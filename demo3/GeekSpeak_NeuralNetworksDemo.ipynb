{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fae0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF Vectorization Summary ===\n",
      "Corpus size: 42 documents\n",
      "Vocabulary size: 514\n",
      "Input dimension (X): (42, 514)\n",
      "Mean feature variance across samples: 0.596522\n",
      "------------------------------------------------------------\n",
      "Lowest-IDF (common) tokens:\n",
      "  im                             2.9694\n",
      "  time                           3.1518\n",
      "  bad                            3.3749\n",
      "  best                           3.3749\n",
      "  hour                           3.3749\n",
      "  purchase                       3.3749\n",
      "  service                        3.3749\n",
      "  absolutely                     3.6626\n",
      "  bug                            3.6626\n",
      "  complete                       3.6626\n",
      "\n",
      "Highest-IDF (rare) tokens:\n",
      "  well expect                    4.0681\n",
      "  wipe                           4.0681\n",
      "  wipe crash                     4.0681\n",
      "  work fine                      4.0681\n",
      "  work fine time                 4.0681\n",
      "  work perfectly                 4.0681\n",
      "  worst                          4.0681\n",
      "  worst ever                     4.0681\n",
      "  year                           4.0681\n",
      "  year flawless                  4.0681\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sample TF-IDF Vector Diagnostics ---\n",
      "\n",
      "Text 1: 'I love this product, it's perfect and exceeded my expectations!'\n",
      "Top weighted tokens:\n",
      "   exceed                         4.0681\n",
      "   exceed expectation             4.0681\n",
      "   expectation                    4.0681\n",
      "   love                           4.0681\n",
      "   love product                   4.0681\n",
      "   love product perfect           4.0681\n",
      "   perfect                        4.0681\n",
      "   perfect exceed                 4.0681\n",
      "   perfect exceed expectation     4.0681\n",
      "   product perfect                4.0681\n",
      "\n",
      "Text 2: 'This application is absolutely terrible and unusable, a complete waste of time.'\n",
      "Top weighted tokens:\n",
      "   absolutely terrible            4.0681\n",
      "   absolutely terrible unusable   4.0681\n",
      "   application                    4.0681\n",
      "   application absolutely         4.0681\n",
      "   application absolutely terrible 4.0681\n",
      "   complete waste                 4.0681\n",
      "   complete waste time            4.0681\n",
      "   terrible                       4.0681\n",
      "   terrible unusable              4.0681\n",
      "   terrible unusable complete     4.0681\n",
      "\n",
      "Text 3: 'It works fine most of the time, no major issues, just average performance.'\n",
      "Top weighted tokens:\n",
      "   average                        4.0681\n",
      "   average performance            4.0681\n",
      "   fine time                      4.0681\n",
      "   fine time major                4.0681\n",
      "   issue average                  4.0681\n",
      "   issue average performance      4.0681\n",
      "   major issue                    4.0681\n",
      "   major issue average            4.0681\n",
      "   performance                    4.0681\n",
      "   time major                     4.0681\n",
      "------------------------------------------------------------\n",
      "Training data shape: (33, 514), Testing data shape: (9, 514)\n",
      "Input feature dimension for model: 514\n",
      "------------------------------------------------------------\n",
      "Starting training on 33 samples for 500 epochs (LR=0.005)...\n",
      "Epoch [100/500], Loss: 0.0088\n",
      "Epoch [200/500], Loss: 0.0082\n",
      "Epoch [300/500], Loss: 0.0084\n",
      "Epoch [400/500], Loss: 0.0074\n",
      "Epoch [500/500], Loss: 0.0072\n",
      "Training Complete.\n",
      "------------------------------------------------------------\n",
      "\n",
      "**Inference Predictions (Score range: -1.0 to 1.0):**\n",
      "\n",
      "Message: 'I was completely blown away by the speed and elegant design. A masterpiece!'\n",
      "Expected: +0.99 | Predicted: +0.6314\n",
      "\n",
      "Message: 'The update ruined the app's functionality; it's slow, buggy, and completely unusable now.'\n",
      "Expected: -0.95 | Predicted: +0.7184\n",
      "\n",
      "Message: 'The meeting notes were recorded, but they didn't offer any constructive feedback.'\n",
      "Expected: +0.05 | Predicted: -0.2336\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# --- Ensure NLTK resources are downloaded ---\n",
    "for resource in [\n",
    "    \"punkt\",\n",
    "    \"wordnet\",\n",
    "    \"stopwords\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{resource}\")\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "# --- 1. DATASET SETUP ---\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I love this product, it's perfect and exceeded my expectations!\",\n",
    "        \"This application is absolutely terrible and unusable, a complete waste of time.\",\n",
    "        \"It works fine most of the time, no major issues, just average performance.\",\n",
    "        \"I’m so disappointed with the lack of features and constant bugs.\",\n",
    "        \"Absolutely fantastic experience, top-notch support and incredibly quick resolution!\",\n",
    "        \"Horrible service! I waited over an hour for a response and got no help.\",\n",
    "        \"Not bad at all, could be better but it serves its basic purpose well.\",\n",
    "        \"Worst thing ever, I'm canceling my subscription right now, I'm furious.\",\n",
    "        \"Great help from support, they were very prompt, efficient, and friendly.\",\n",
    "        \"Okay I guess, nothing special about it, quite neutral actually.\",\n",
    "        \"The service was bad and my issue wasn't fixed.\",\n",
    "        \"I have no opinion on the matter, it just exists.\",\n",
    "        \"The user interface is clean, easy to navigate, and highly intuitive.\",\n",
    "        \"It crashes every time I open the settings menu—completely broken software.\",\n",
    "        \"It performs the core function, but the load times are truly unacceptable.\",\n",
    "        \"I'm cautiously optimistic about the new features; they seem promising.\",\n",
    "        \"I'm giving this a neutral score because I haven't used it enough to form an opinion.\",\n",
    "        \"The price is a bit high for what it offers, making it a marginal value.\",\n",
    "        \"Honestly, it's the best software update I've seen all year. Flawless!\",\n",
    "        \"It was merely adequate; I encountered several minor inconveniences but nothing major.\",\n",
    "        \"The customer service representative was rude, arrogant, and unhelpful.\",\n",
    "        \"I found a bug, but otherwise, the experience was quite positive and speedy.\",\n",
    "        \"This is highly functional, completely reliable, and I recommend it to everyone.\",\n",
    "        \"I am so angry; the data I spent hours collecting was completely wiped out by the crash.\",\n",
    "        \"The setup process was slightly confusing, leading to some early frustration.\",\n",
    "        \"After a few hours of tinkering, it turned out to be exactly what I needed. Solid purchase.\",\n",
    "        \"It's loud, bulky, and poorly designed. I regret this purchase.\",\n",
    "        \"I was pleasantly surprised by the quality, which was much better than I expected.\",\n",
    "        \"The documentation is non-existent, making it impossible to debug any problems.\",\n",
    "        \"It’s totally fine, not the best, but I can't complain for the low price.\",\n",
    "        \"This is amazing.\",\n",
    "        \"I hate this.\",\n",
    "        \"It works perfectly.\",\n",
    "        \"A total failure.\",\n",
    "        \"I am so happy with the results.\",\n",
    "        \"This product is trash.\",\n",
    "        \"Excellent.\",\n",
    "        \"Worst ever.\",\n",
    "        \"Simply the best purchase.\",\n",
    "        \"A complete disaster.\",\n",
    "        \"I'm thrilled!\",\n",
    "        \"Total waste of money.\",\n",
    "    ],\n",
    "    \"label\": [\n",
    "        1.0, -0.9, 0.3, -0.85, 1.0, -1.0, 0.4, -1.0, 0.85, 0.0, -0.8, 0.0,\n",
    "        0.95, -0.95, -0.5, 0.55, 0.0, -0.25, 1.0, 0.1, -0.75, 0.6, 0.9, -0.99,\n",
    "        -0.4, 0.7, -0.8, 0.8, -0.7, 0.25, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0,\n",
    "        1.0, -1.0, 1.0, -1.0, 1.0, -1.0\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- 2. PREPROCESSING & VECTORIZATION ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    tag_map = {'J': wordnet.ADJ, 'V': wordnet.VERB, 'N': wordnet.NOUN, 'R': wordnet.ADV}\n",
    "    return tag_map.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "def lemmatize_tokenizer(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = [w for w in text.split() if w not in stop_words and len(w) > 1]\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged]\n",
    "    return lemmatized\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=lemmatize_tokenizer,\n",
    "    ngram_range=(1, 3),\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    preprocessor=None,\n",
    "    norm=None\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"text\"]).toarray()\n",
    "y = df[\"label\"].values.astype(np.float32).reshape(-1, 1)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# --- Vectorization Diagnostics ---\n",
    "print(\"\\n=== TF-IDF Vectorization Summary ===\")\n",
    "print(f\"Corpus size: {len(df)} documents\")\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"Input dimension (X): {X.shape}\")\n",
    "print(f\"Mean feature variance across samples: {np.mean(np.std(X, axis=1)):.6f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "idf_values = vectorizer.idf_\n",
    "idf_map = sorted(zip(feature_names, idf_values), key=lambda x: x[1])\n",
    "print(\"Lowest-IDF (common) tokens:\")\n",
    "for tok, val in idf_map[:10]:\n",
    "    print(f\"  {tok:<30s} {val:.4f}\")\n",
    "print(\"\\nHighest-IDF (rare) tokens:\")\n",
    "for tok, val in idf_map[-10:]:\n",
    "    print(f\"  {tok:<30s} {val:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n--- Sample TF-IDF Vector Diagnostics ---\")\n",
    "sample_indices = [0, 1, 2]\n",
    "for i in sample_indices:\n",
    "    text = df[\"text\"].iloc[i]\n",
    "    vec = X[i]\n",
    "    nonzero_idx = np.where(vec > 0)[0]\n",
    "    top_tokens = sorted(\n",
    "        [(feature_names[j], vec[j]) for j in nonzero_idx],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "    print(f\"\\nText {i+1}: '{text}'\")\n",
    "    print(\"Top weighted tokens:\")\n",
    "    for tok, val in top_tokens:\n",
    "        print(f\"   {tok:<30s} {val:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "INPUT_SIZE = X_train.shape[1]\n",
    "print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "print(f\"Input feature dimension for model: {INPUT_SIZE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 3. MODEL DEFINITION ---\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.tanh(self.fc3(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentimentNet(INPUT_SIZE).to(device)\n",
    "\n",
    "# --- 4. TRAINING LOOP ---\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 0.005\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "\n",
    "print(f\"Starting training on {len(X_train)} samples for {EPOCHS} epochs (LR={LEARNING_RATE})...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "print(\"Training Complete.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 5. INFERENCE ---\n",
    "model.eval()\n",
    "test_cases = [\n",
    "    (\"I was completely blown away by the speed and elegant design. A masterpiece!\", 0.99),\n",
    "    (\"The update ruined the app's functionality; it's slow, buggy, and completely unusable now.\", -0.95),\n",
    "    (\"The meeting notes were recorded, but they didn't offer any constructive feedback.\", 0.05)\n",
    "]\n",
    "\n",
    "def predict_sentiment(text, vectorizer, model, device):\n",
    "    new_vec = vectorizer.transform([text]).toarray()\n",
    "    new_tensor = torch.FloatTensor(new_vec).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(new_tensor).squeeze().item()\n",
    "    return prediction\n",
    "\n",
    "print(\"\\n**Inference Predictions (Score range: -1.0 to 1.0):**\")\n",
    "for text, expected in test_cases:\n",
    "    prediction = predict_sentiment(text, vectorizer, model, device)\n",
    "    print(f\"\\nMessage: '{text}'\")\n",
    "    print(f\"Expected: {expected:+.2f} | Predicted: {prediction:+.4f}\")\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c05433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18a6ee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Inference Predictions (Score range: -1.0 to 1.0):**\n",
      "\n",
      "Message: 'I was completely blown away by the speed and elegant design. A masterpiece!'\n",
      "Expected: Positive | Predicted Score: +0.6314 | Predicted Sentiment: Positive\n",
      "\n",
      "Message: 'The app's functionality is slow, buggy, and completely unusable now. Complete waste of time'\n",
      "Expected: Negative | Predicted Score: -0.9997 | Predicted Sentiment: Negative\n",
      "\n",
      "Message: 'The meeting notes were recorded.'\n",
      "Expected: Neutral | Predicted Score: -0.2858 | Predicted Sentiment: Neutral\n",
      "\n",
      "Message: 'I have no preference about the color of the application.'\n",
      "Expected: Neutral | Predicted Score: -0.7570 | Predicted Sentiment: Negative\n",
      "\n",
      "Message: 'The service was okay, and I don't have an opinion.'\n",
      "Expected: Neutral | Predicted Score: -0.9638 | Predicted Sentiment: Negative\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (\"I was completely blown away by the speed and elegant design. A masterpiece!\", \"Positive\"),\n",
    "    (\"The app's functionality is slow, buggy, and completely unusable now. Complete waste of time\", \"Negative\"),\n",
    "    (\"The meeting notes were recorded.\", \"Neutral\"),\n",
    "    (\"I have no preference about the color of the application.\", \"Neutral\"),\n",
    "    (\"The service was okay, and I don't have an opinion.\", \"Neutral\"),\n",
    "]\n",
    "\n",
    "def predict_sentiment(text, vectorizer, model, device):\n",
    "    new_vec = vectorizer.transform([text]).toarray()\n",
    "    new_tensor = torch.FloatTensor(new_vec).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(new_tensor).squeeze().item()\n",
    "    return prediction\n",
    "\n",
    "def classify_sentiment(score):\n",
    "    \"\"\"Convert numeric score to categorical label.\"\"\"\n",
    "    if score > 0.35:\n",
    "        return \"Positive\"\n",
    "    elif score < -0.35:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "print(\"\\n**Inference Predictions (Score range: -1.0 to 1.0):**\")\n",
    "for text, expected in test_cases:\n",
    "    score = predict_sentiment(text, vectorizer, model, device)\n",
    "    sentiment_label = classify_sentiment(score)\n",
    "    print(f\"\\nMessage: '{text}'\")\n",
    "    print(f\"Expected: {expected} | Predicted Score: {score:+.4f} | Predicted Sentiment: {sentiment_label}\")\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8973d3",
   "metadata": {},
   "source": [
    "Setup, Imports, and NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03734304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NLTK resources...\n",
      "Downloading NLTK resource: punkt...\n",
      "Downloading NLTK resource: wordnet...\n",
      "Downloading NLTK resource: averaged_perceptron_tagger...\n",
      "NLTK resources are ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "\n",
    "# Suppress sklearn future warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- Ensure NLTK resources are downloaded ---\n",
    "# This block checks for and downloads the necessary NLTK components \n",
    "# (tokenization, word list, part-of-speech tagging data) needed for lemmatization.\n",
    "print(\"Checking for NLTK resources...\")\n",
    "for resource in [\n",
    "    \"punkt\",\n",
    "    \"wordnet\",\n",
    "    \"stopwords\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{resource}\")\n",
    "    except LookupError:\n",
    "        print(f\"Downloading NLTK resource: {resource}...\")\n",
    "        nltk.download(resource)\n",
    "print(\"NLTK resources are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bacf1",
   "metadata": {},
   "source": [
    "Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f4a66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 42 samples.\n",
      "\n",
      "First 5 samples:\n",
      "                                                text  label\n",
      "0  I love this product, it's perfect and exceeded...   1.00\n",
      "1  This application is absolutely terrible and un...  -0.90\n",
      "2  It works fine most of the time, no major issue...   0.30\n",
      "3  I’m so disappointed with the lack of features ...  -0.85\n",
      "4  Absolutely fantastic experience, top-notch sup...   1.00\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DATASET SETUP ---\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I love this product, it's perfect and exceeded my expectations!\",\n",
    "        \"This application is absolutely terrible and unusable, a complete waste of time.\",\n",
    "        \"It works fine most of the time, no major issues, just average performance.\",\n",
    "        \"I’m so disappointed with the lack of features and constant bugs.\",\n",
    "        \"Absolutely fantastic experience, top-notch support and incredibly quick resolution!\",\n",
    "        \"Horrible service! I waited over an hour for a response and got no help.\",\n",
    "        \"Not bad at all, could be better but it serves its basic purpose well.\",\n",
    "        \"Worst thing ever, I'm canceling my subscription right now, I'm furious.\",\n",
    "        \"Great help from support, they were very prompt, efficient, and friendly.\",\n",
    "        \"Okay I guess, nothing special about it, quite neutral actually.\",\n",
    "        \"The service was bad and my issue wasn't fixed.\",\n",
    "        \"I have no opinion on the matter, it just exists.\",\n",
    "        \"The user interface is clean, easy to navigate, and highly intuitive.\",\n",
    "        \"It crashes every time I open the settings menu—completely broken software.\",\n",
    "        \"It performs the core function, but the load times are truly unacceptable.\",\n",
    "        \"I'm cautiously optimistic about the new features; they seem promising.\",\n",
    "        \"I'm giving this a neutral score because I haven't used it enough to form an opinion.\",\n",
    "        \"The price is a bit high for what it offers, making it a marginal value.\",\n",
    "        \"Honestly, it's the best software update I've seen all year. Flawless!\",\n",
    "        \"It was merely adequate; I encountered several minor inconveniences but nothing major.\",\n",
    "        \"The customer service representative was rude, arrogant, and unhelpful.\",\n",
    "        \"I found a bug, but otherwise, the experience was quite positive and speedy.\",\n",
    "        \"This is highly functional, completely reliable, and I recommend it to everyone.\",\n",
    "        \"I am so angry; the data I spent hours collecting was completely wiped out by the crash.\",\n",
    "        \"The setup process was slightly confusing, leading to some early frustration.\",\n",
    "        \"After a few hours of tinkering, it turned out to be exactly what I needed. Solid purchase.\",\n",
    "        \"It's loud, bulky, and poorly designed. I regret this purchase.\",\n",
    "        \"I was pleasantly surprised by the quality, which was much better than I expected.\",\n",
    "        \"The documentation is non-existent, making it impossible to debug any problems.\",\n",
    "        \"It’s totally fine, not the best, but I can't complain for the low price.\",\n",
    "        \"This is amazing.\",\n",
    "        \"I hate this.\",\n",
    "        \"It works perfectly.\",\n",
    "        \"A total failure.\",\n",
    "        \"I am so happy with the results.\",\n",
    "        \"This product is trash.\",\n",
    "        \"Excellent.\",\n",
    "        \"Worst ever.\",\n",
    "        \"Simply the best purchase.\",\n",
    "        \"A complete disaster.\",\n",
    "        \"I'm thrilled!\",\n",
    "        \"Total waste of money.\",\n",
    "    ],\n",
    "    \"label\": [\n",
    "        1.0, -0.9, 0.3, -0.85, 1.0, -1.0, 0.4, -1.0, 0.85, 0.0, -0.8, 0.0,\n",
    "        0.95, -0.95, -0.5, 0.55, 0.0, -0.25, 1.0, 0.1, -0.75, 0.6, 0.9, -0.99,\n",
    "        -0.4, 0.7, -0.8, 0.8, -0.7, 0.25, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0,\n",
    "        1.0, -1.0, 1.0, -1.0, 1.0, -1.0\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display a sample of the data\n",
    "print(f\"Dataset loaded with {len(df)} samples.\")\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5aef1e",
   "metadata": {},
   "source": [
    "Text Preprocessing, Vectorization, and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832552bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest-IDF (common) tokens:\n",
      "  im                             2.9694\n",
      "  time                           3.1518\n",
      "  bad                            3.3749\n",
      "  best                           3.3749\n",
      "  hour                           3.3749\n",
      "  purchase                       3.3749\n",
      "  service                        3.3749\n",
      "  absolutely                     3.6626\n",
      "  bug                            3.6626\n",
      "  complete                       3.6626\n",
      "\n",
      "=== TF-IDF Vectorization Summary ===\n",
      "Corpus size: 42 documents\n",
      "Vocabulary size: 514 (Max features was 1000)\n",
      "Input dimension (X): (42, 514)\n",
      "------------------------------------------------------------\n",
      "Training data shape: (33, 514), Testing data shape: (9, 514)\n",
      "Input feature dimension for model: 514\n"
     ]
    }
   ],
   "source": [
    "# --- 2. PREPROCESSING & VECTORIZATION ---\n",
    "# Initialize lemmatizer and set of English stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Helper function to map NLTK's Part-of-Speech tag to WordNet's POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map NLTK POS tag to WordNet POS tag for accurate lemmatization.\"\"\"\n",
    "    tag_map = {'J': wordnet.ADJ, 'V': wordnet.VERB, 'N': wordnet.NOUN, 'R': wordnet.ADV}\n",
    "    # Default to NOUN if the tag is not recognized\n",
    "    return tag_map.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "# Custom tokenizer for TF-IDF that performs cleaning, stop word removal, and lemmatization\n",
    "def lemmatize_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Cleans text, removes stop words, and lemmatizes tokens using POS tags.\n",
    "    \"\"\"\n",
    "    # Remove punctuation/non-word characters and convert to lower case\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Tokenize and remove stop words and single-character tokens\n",
    "    tokens = [w for w in text.split() if w not in stop_words and len(w) > 1]\n",
    "    # Get POS tags for tokens\n",
    "    tagged = pos_tag(tokens)\n",
    "    # Lemmatize tokens based on their POS tag\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged]\n",
    "    return lemmatized\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,           # Max number of features (tokens/n-grams) to keep\n",
    "    tokenizer=lemmatize_tokenizer, # Use the custom lemmatizing tokenizer\n",
    "    ngram_range=(1, 3),          # Include unigrams, bigrams, and trigrams\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,           # Scaling factor to prevent highly frequent terms from dominating\n",
    "    preprocessor=None,\n",
    "    norm='l2' # Add L2 normalization for the final vector (common for neural networks)\n",
    ")\n",
    "\n",
    "# Apply vectorization to the 'text' data\n",
    "X = vectorizer.fit_transform(df[\"text\"]).toarray()\n",
    "# Convert labels to NumPy float32 array, suitable for PyTorch regression\n",
    "y = df[\"label\"].values.astype(np.float32).reshape(-1, 1)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "import random\n",
    "idf_values = vectorizer.idf_\n",
    "idf_map = sorted(zip(feature_names, idf_values), key=lambda x: x[1])\n",
    "print(\"IDF tokens:\")\n",
    "for tok, val in idf_map[:10]:\n",
    "    print(f\"  {tok:<30s} {val:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Vectorization Diagnostics ---\n",
    "print(\"\\n=== TF-IDF Vectorization Summary ===\")\n",
    "print(f\"Corpus size: {len(df)} documents\")\n",
    "print(f\"Vocabulary size: {len(feature_names)} (Max features was 1000)\")\n",
    "print(f\"Input dimension (X): {X.shape}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "INPUT_SIZE = X_train.shape[1] # The number of features from the vectorizer\n",
    "print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "print(f\"Input feature dimension for model: {INPUT_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4a5da",
   "metadata": {},
   "source": [
    "Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44188ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on 33 samples for 5000 epochs (LR=0.005) on cuda...\n",
      "Epoch [100/5000], Loss: 0.0004\n",
      "Epoch [200/5000], Loss: 0.0000\n",
      "Epoch [300/5000], Loss: 0.0000\n",
      "Epoch [400/5000], Loss: 0.0001\n",
      "Epoch [500/5000], Loss: 0.0000\n",
      "Epoch [600/5000], Loss: 0.0000\n",
      "Epoch [700/5000], Loss: 0.0000\n",
      "Epoch [800/5000], Loss: 0.0000\n",
      "Epoch [900/5000], Loss: 0.0000\n",
      "Epoch [1000/5000], Loss: 0.0000\n",
      "Epoch [1100/5000], Loss: 0.0000\n",
      "Epoch [1200/5000], Loss: 0.0000\n",
      "Epoch [1300/5000], Loss: 0.0000\n",
      "Epoch [1400/5000], Loss: 0.0000\n",
      "Epoch [1500/5000], Loss: 0.0001\n",
      "Epoch [1600/5000], Loss: 0.0000\n",
      "Epoch [1700/5000], Loss: 0.0000\n",
      "Epoch [1800/5000], Loss: 0.0000\n",
      "Epoch [1900/5000], Loss: 0.0000\n",
      "Epoch [2000/5000], Loss: 0.0000\n",
      "Epoch [2100/5000], Loss: 0.0000\n",
      "Epoch [2200/5000], Loss: 0.0000\n",
      "Epoch [2300/5000], Loss: 0.0000\n",
      "Epoch [2400/5000], Loss: 0.0000\n",
      "Epoch [2500/5000], Loss: 0.0000\n",
      "Epoch [2600/5000], Loss: 0.0000\n",
      "Epoch [2700/5000], Loss: 0.0000\n",
      "Epoch [2800/5000], Loss: 0.0000\n",
      "Epoch [2900/5000], Loss: 0.0000\n",
      "Epoch [3000/5000], Loss: 0.0000\n",
      "Epoch [3100/5000], Loss: 0.0000\n",
      "Epoch [3200/5000], Loss: 0.0000\n",
      "Epoch [3300/5000], Loss: 0.0000\n",
      "Epoch [3400/5000], Loss: 0.0000\n",
      "Epoch [3500/5000], Loss: 0.0000\n",
      "Epoch [3600/5000], Loss: 0.0000\n",
      "Epoch [3700/5000], Loss: 0.0000\n",
      "Epoch [3800/5000], Loss: 0.0000\n",
      "Epoch [3900/5000], Loss: 0.0000\n",
      "Epoch [4000/5000], Loss: 0.0000\n",
      "Epoch [4100/5000], Loss: 0.0000\n",
      "Epoch [4200/5000], Loss: 0.0000\n",
      "Epoch [4300/5000], Loss: 0.0000\n",
      "Epoch [4400/5000], Loss: 0.0000\n",
      "Epoch [4500/5000], Loss: 0.0000\n",
      "Epoch [4600/5000], Loss: 0.0000\n",
      "Epoch [4700/5000], Loss: 0.0000\n",
      "Epoch [4800/5000], Loss: 0.0000\n",
      "Epoch [4900/5000], Loss: 0.0000\n",
      "Epoch [5000/5000], Loss: 0.0000\n",
      "Training Complete.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 3. MODEL DEFINITION ---\n",
    "class SentimentNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Feed-Forward Neural Network (FFNN) for regression.\n",
    "    The final activation is Tanh, which squashes the output to a range of [-1, 1],\n",
    "    matching the label range for sentiment scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        # Input layer (size is the number of TF-IDF features)\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output layer (size 1 for the single sentiment score)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        # Tanh activation squashes the final output to a range of [-1, 1]\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.tanh(self.fc3(x))\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentimentNet(INPUT_SIZE).to(device)\n",
    "\n",
    "# --- 4. TRAINING LOOP SETUP ---\n",
    "EPOCHS = 5000\n",
    "LEARNING_RATE = 0.005\n",
    "# Mean Squared Error (MSE) is used as the loss function for regression tasks\n",
    "criterion = nn.MSELoss() \n",
    "# Adam is a popular optimization algorithm\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensors and move them to the selected device\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "print(f\"Starting training on {len(X_train)} samples for {EPOCHS} epochs (LR={LEARNING_RATE}) on {device}...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Clear previous gradients\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Print loss periodically to monitor progress\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "print(\"Training Complete.\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d655d1f",
   "metadata": {},
   "source": [
    "Inference and Testing on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b67ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Inference Predictions (Score range: -1.0 to 1.0):**\n",
      "\n",
      "Message: 'I was completely blown away by the speed and elegant design. A masterpiece!'\n",
      "Expected: Positive | Predicted Score: +0.3337 | Predicted Sentiment: Neutral\n",
      "\n",
      "Message: 'The app's functionality is slow, buggy, and completely unusable now. Complete waste of time'\n",
      "Expected: Negative | Predicted Score: -0.8515 | Predicted Sentiment: Negative\n",
      "\n",
      "Message: 'The meeting notes were recorded. I am not sure if they are useful'\n",
      "Expected: Neutral | Predicted Score: +0.1849 | Predicted Sentiment: Neutral\n",
      "\n",
      "Message: 'I have no preference about the color of the application.'\n",
      "Expected: Neutral | Predicted Score: -0.2969 | Predicted Sentiment: Neutral\n",
      "\n",
      "Message: 'The service was well taken care of, and I really liked it.'\n",
      "Expected: Positive | Predicted Score: -0.0105 | Predicted Sentiment: Neutral\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (\"I was completely blown away by the speed and elegant design. A masterpiece!\", \"Positive\"),\n",
    "    (\"The app's functionality is slow, buggy, and completely unusable now. Complete waste of time\", \"Negative\"),\n",
    "    (\"The meeting notes were recorded. I am not sure if they are useful\", \"Neutral\"),\n",
    "    (\"I have no preference about the color of the application.\", \"Neutral\"),\n",
    "    (\"The service was well taken care of, and I really liked it.\", \"Positive\"),\n",
    "]\n",
    "\n",
    "def predict_sentiment(text, vectorizer, model, device):\n",
    "    new_vec = vectorizer.transform([text]).toarray()\n",
    "    new_tensor = torch.FloatTensor(new_vec).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(new_tensor).squeeze().item()\n",
    "    return prediction\n",
    "\n",
    "def classify_sentiment(score):\n",
    "    \"\"\"Convert numeric score to categorical label.\"\"\"\n",
    "    if score > 0.35:\n",
    "        return \"Positive\"\n",
    "    elif score < -0.35:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "print(\"\\n**Inference Predictions (Score range: -1.0 to 1.0):**\")\n",
    "for text, expected in test_cases:\n",
    "    score = predict_sentiment(text, vectorizer, model, device)\n",
    "    sentiment_label = classify_sentiment(score)\n",
    "    print(f\"\\nMessage: '{text}'\")\n",
    "    print(f\"Expected: {expected} | Predicted Score: {score:+.4f} | Predicted Sentiment: {sentiment_label}\")\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa08b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-pytorch-env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
