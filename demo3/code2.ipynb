{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d004907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total messages loaded: 42\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# NOTE: Ensure you have downloaded the required NLTK resources:\n",
    "# ```python\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# ```\n",
    "\n",
    "# # --- 1. GPU/Device Setup ---\n",
    "# **PURPOSE:** Check for CUDA availability to utilize GPU acceleration, defaulting to CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# # --- Dataset Definition ---\n",
    "# --- 1. Data Setup (42 Examples: 30 Original + 12 Easy) ---\n",
    "data = {\n",
    "    \"text\": [\n",
    "        # --- Original Varied Examples (30) ---\n",
    "        \"I love this product, it's perfect and exceeded my expectations!\",\n",
    "        \"This application is absolutely terrible and unusable, a complete waste of time.\",\n",
    "        \"It works fine most of the time, no major issues, just average performance.\",\n",
    "        \"I’m so disappointed with the lack of features and constant bugs.\",\n",
    "        \"Absolutely fantastic experience, top-notch support and incredibly quick resolution!\",\n",
    "        \"Horrible service! I waited over an hour for a response and got no help.\",\n",
    "        \"Not bad at all, could be better but it serves its basic purpose well.\",\n",
    "        \"Worst thing ever, I'm canceling my subscription right now, I'm furious.\",\n",
    "        \"Great help from support, they were very prompt, efficient, and friendly.\",\n",
    "        \"Okay I guess, nothing special about it, quite neutral actually.\",\n",
    "        \"The service was bad and my issue wasn't fixed.\",\n",
    "        \"I have no opinion on the matter, it just exists.\",\n",
    "        \"The user interface is clean, easy to navigate, and highly intuitive.\",\n",
    "        \"It crashes every time I open the settings menu—completely broken software.\",\n",
    "        \"It performs the core function, but the load times are truly unacceptable.\",\n",
    "        \"I'm cautiously optimistic about the new features; they seem promising.\",\n",
    "        \"I'm giving this a neutral score because I haven't used it enough to form an opinion.\",\n",
    "        \"The price is a bit high for what it offers, making it a marginal value.\",\n",
    "        \"Honestly, it's the best software update I've seen all year. Flawless!\",\n",
    "        \"It was merely adequate; I encountered several minor inconveniences but nothing major.\",\n",
    "        \"The customer service representative was rude, arrogant, and unhelpful.\",\n",
    "        \"I found a bug, but otherwise, the experience was quite positive and speedy.\",\n",
    "        \"This is highly functional, completely reliable, and I recommend it to everyone.\",\n",
    "        \"I am so angry; the data I spent hours collecting was completely wiped out by the crash.\",\n",
    "        \"The setup process was slightly confusing, leading to some early frustration.\",\n",
    "        \"After a few hours of tinkering, it turned out to be exactly what I needed. Solid purchase.\",\n",
    "        \"It's loud, bulky, and poorly designed. I regret this purchase.\",\n",
    "        \"I was pleasantly surprised by the quality, which was much better than I expected.\",\n",
    "        \"The documentation is non-existent, making it impossible to debug any problems.\",\n",
    "        \"It’s totally fine, not the best, but I can't complain for the low price.\",\n",
    "        \"This is amazing.\",                                  # 1.0\n",
    "        \"I hate this.\",                                     # -1.0\n",
    "        \"It works perfectly.\",                              # 1.0\n",
    "        \"A total failure.\",                                 # -1.0\n",
    "        \"I am so happy with the results.\",                  # 1.0\n",
    "        \"This product is trash.\",                           # -1.0\n",
    "        \"Excellent.\",                                       # 1.0\n",
    "        \"Worst ever.\",                                      # -1.0\n",
    "        \"Simply the best purchase.\",                        # 1.0\n",
    "        \"A complete disaster.\",                             # -1.0\n",
    "        \"I'm thrilled!\",                                    # 1.0\n",
    "        \"Total waste of money.\",                            # -1.0\n",
    "    ],\n",
    "    \"label\": [\n",
    "        1.0, -0.9, 0.3, -0.85, 1.0, -1.0, 0.4, -1.0, 0.85, 0.0, -0.8, 0.0,\n",
    "        0.95, -0.95, -0.5, 0.55, 0.0, -0.25, 1.0, 0.1, -0.75, 0.6, 0.9, -0.99,\n",
    "        -0.4, 0.7, -0.8, 0.8, -0.7, 0.25,\n",
    "        1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Total messages loaded: {len(df)}\")\n",
    "print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8b70d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **Vectorization (TF-IDF) Concept with N-grams:**\n",
      "  - **Final Vocabulary Size (Input Dim):** 515\n",
      "  - **Example Vocabulary (Features):**\n",
      "    - N-grams Examples: ['absolutely fantastic', 'absolutely fantastic experience', 'absolutely terrible', 'absolutely', 'actually', 'adequate']\n",
      "  - **Example Bigram/Trigram Features in First Message:** {'exceed expectations': 0.29098067449497644, 'love product': 0.29098067449497644, 'love product perfect': 0.29098067449497644, 'perfect exceed': 0.29098067449497644, 'perfect exceed expectations': 0.29098067449497644, 'product perfect': 0.29098067449497644, 'product perfect exceed': 0.29098067449497644}\n",
      "--------------------------------------------------\n",
      " **Data Split and Dimension:**\n",
      "  - Training data shape: (33, 515)\n",
      "  - Testing data shape: (9, 515)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # --- 2. Text Preprocessing: Lemmatization and N-grams ---\n",
    "\n",
    "# **Parameter:** Lemmatizer initialization (NLTK WordNet)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# **Parameter:** Define common English stop words for filtering\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Custom tokenizer for TfidfVectorizer.\n",
    "    - **Cleaning:** Removes punctuation and converts to lowercase.\n",
    "    - **Filtering:** Removes common stop words and single-letter tokens.\n",
    "    - **Lemmatization:** Reduces words to their base or root form (e.g., 'waiting' -> 'wait').\n",
    "    \"\"\"\n",
    "    original_text = text\n",
    "    \n",
    "    # 1. Cleaning and Lowercasing\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    # 2. Tokenization, Stop Word Removal, and Lemmatization\n",
    "    tokens_filtered = []\n",
    "    for w in text.split():\n",
    "        if w not in stop_words and len(w) > 1: # Filter common stop words and single letters\n",
    "            # **Lemmatization:** Assuming verb tense ('v') for aggressive normalization\n",
    "            tokens_filtered.append(lemmatizer.lemmatize(w, pos='v'))\n",
    "            \n",
    "    # Detailed Print Statements for NLP Concept Stages:\n",
    "    if len(df['text']) > 0 and original_text == df['text'].iloc[0]:\n",
    "        print(f\" NLP Pipeline Example (First Message): '{original_text}'\")\n",
    "        print(f\"  - **Initial Tokens:** {text.split()[:10]}...\")\n",
    "        print(f\"  - **Lemmatization & Filtering (Unigrams):** {tokens_filtered[:10]}...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    return tokens_filtered\n",
    "\n",
    "# 1. Initialize the vectorizer\n",
    "# **HYPERPARAMETER/TOOL:** TfidfVectorizer (Term Frequency-Inverse Document Frequency)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # **Hyperparameter:** max_features - Limits vocabulary size, managing model complexity.\n",
    "    max_features=1000, \n",
    "    # **Parameter:** tokenizer - Uses our custom function for cleaning/lemmatization.\n",
    "    tokenizer=lemmatize_tokenizer,\n",
    "    # **Hyperparameter:** ngram_range - Includes single words (1), two-word phrases (2), and three-word phrases (3).\n",
    "    # This captures **context** like \"not bad\" or \"waste of time\".\n",
    "    ngram_range=(1, 3), \n",
    "    preprocessor=None \n",
    ")\n",
    "\n",
    "# 2. **Vectorization:** Fit and transform text data to numerical features\n",
    "X = vectorizer.fit_transform(df[\"text\"]).toarray()\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Detailed Print Statement for Vectorization\n",
    "print(\" **Vectorization (TF-IDF) Concept with N-grams:**\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"  - **Final Vocabulary Size (Input Dim):** {X.shape[1]}\")\n",
    "print(f\"  - **Example Vocabulary (Features):**\")\n",
    "example_features = [f for f in feature_names if len(f.split()) > 1][:3] + [f for f in feature_names if len(f.split()) == 1][:3]\n",
    "print(f\"    - N-grams Examples: {example_features}\")\n",
    "\n",
    "# Example features from the first message\n",
    "sample_vector = X[0]\n",
    "non_zero_indices = np.nonzero(sample_vector)[0]\n",
    "sample_features = {feature_names[i]: sample_vector[i] for i in non_zero_indices if len(feature_names[i].split()) > 1}\n",
    "print(f\"  - **Example Bigram/Trigram Features in First Message:** {sample_features}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. **Data Split**\n",
    "# **Hyperparameter:** test_size=0.2 (20% of data reserved for testing/evaluation)\n",
    "# **Parameter:** random_state=42 (Ensures split is reproducible)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "print(f\" **Data Split and Dimension:**\")\n",
    "print(f\"  - Training data shape: {X_train.shape}\")\n",
    "print(f\"  - Testing data shape: {X_test.shape}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cd0441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **Model Architecture and Hyperparameters:**\n",
      "  - **Input Dimension (Features):** 515\n",
      "  - **Hidden Layers (Nodes):** [128, 64]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # --- 3. Neural Network Definition and Setup ---\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Feedforward Network for Sentiment Regression.\n",
    "    - Takes a sparse TF-IDF vector as input.\n",
    "    - Outputs a single scalar sentiment score between -1.0 and 1.0.\n",
    "    \"\"\"\n",
    "    # **Parameter:** input_dim - The size of the input vector (vocabulary size from TF-IDF).\n",
    "    # **Hyperparameter:** hidden_dims_list - List defining hidden layer node counts.\n",
    "    def __init__(self, input_dim, hidden_dims_list):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        \n",
    "        layer_dims = [input_dim] + hidden_dims_list + [1]\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            # **Component:** nn.Linear (Fully Connected Layer)\n",
    "            self.layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Pass through hidden layers\n",
    "        for layer in self.layers[:-1]:\n",
    "            # **Activation Function (Hidden Layers):** F.relu (Rectified Linear Unit)\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        # **Activation Function (Output Layer):** torch.tanh (Hyperbolic Tangent)\n",
    "        # This function squashes the final output into the desired [-1, 1] range.\n",
    "        return torch.tanh(self.layers[-1](x))\n",
    "\n",
    "# **Hyperparameter:** Hidden Layer Sizes (Architecture)\n",
    "HIDDEN_DIMS = [128, 64] # Two hidden layers\n",
    "model = SentimentNet(INPUT_DIM, HIDDEN_DIMS)\n",
    "\n",
    "print(\" **Model Architecture and Hyperparameters:**\")\n",
    "print(f\"  - **Input Dimension (Features):** {INPUT_DIM}\")\n",
    "print(f\"  - **Hidden Layers (Nodes):** {HIDDEN_DIMS}\")\n",
    "print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9155555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **Starting training for 2000 epochs on cuda...**\n",
      "  - Epoch [20/2000], Loss: 0.0220\n",
      "  - Epoch [40/2000], Loss: 0.0097\n",
      "  - Epoch [60/2000], Loss: 0.0037\n",
      "  - Epoch [80/2000], Loss: 0.0025\n",
      "  - Epoch [100/2000], Loss: 0.0011\n",
      "  - Epoch [120/2000], Loss: 0.0004\n",
      "  - Epoch [140/2000], Loss: 0.0001\n",
      "  - Epoch [160/2000], Loss: 0.0001\n",
      "  - Epoch [180/2000], Loss: 0.0000\n",
      "  - Epoch [200/2000], Loss: 0.0000\n",
      "  - Epoch [220/2000], Loss: 0.0000\n",
      "  - Epoch [240/2000], Loss: 0.0000\n",
      "  - Epoch [260/2000], Loss: 0.0000\n",
      "  - Epoch [280/2000], Loss: 0.0000\n",
      "  - Epoch [300/2000], Loss: 0.0000\n",
      "  - Epoch [320/2000], Loss: 0.0000\n",
      "  - Epoch [340/2000], Loss: 0.0000\n",
      "  - Epoch [360/2000], Loss: 0.0000\n",
      "  - Epoch [380/2000], Loss: 0.0000\n",
      "  - Epoch [400/2000], Loss: 0.0000\n",
      "  - Epoch [420/2000], Loss: 0.0000\n",
      "  - Epoch [440/2000], Loss: 0.0000\n",
      "  - Epoch [460/2000], Loss: 0.0000\n",
      "  - Epoch [480/2000], Loss: 0.0000\n",
      "  - Epoch [500/2000], Loss: 0.0000\n",
      "  - Epoch [520/2000], Loss: 0.0000\n",
      "  - Epoch [540/2000], Loss: 0.0000\n",
      "  - Epoch [560/2000], Loss: 0.0000\n",
      "  - Epoch [580/2000], Loss: 0.0000\n",
      "  - Epoch [600/2000], Loss: 0.0000\n",
      "  - Epoch [620/2000], Loss: 0.0000\n",
      "  - Epoch [640/2000], Loss: 0.0000\n",
      "  - Epoch [660/2000], Loss: 0.0000\n",
      "  - Epoch [680/2000], Loss: 0.0000\n",
      "  - Epoch [700/2000], Loss: 0.0000\n",
      "  - Epoch [720/2000], Loss: 0.0000\n",
      "  - Epoch [740/2000], Loss: 0.0000\n",
      "  - Epoch [760/2000], Loss: 0.0000\n",
      "  - Epoch [780/2000], Loss: 0.0000\n",
      "  - Epoch [800/2000], Loss: 0.0000\n",
      "  - Epoch [820/2000], Loss: 0.0000\n",
      "  - Epoch [840/2000], Loss: 0.0000\n",
      "  - Epoch [860/2000], Loss: 0.0000\n",
      "  - Epoch [880/2000], Loss: 0.0002\n",
      "  - Epoch [900/2000], Loss: 0.0000\n",
      "  - Epoch [920/2000], Loss: 0.0000\n",
      "  - Epoch [940/2000], Loss: 0.0000\n",
      "  - Epoch [960/2000], Loss: 0.0000\n",
      "  - Epoch [980/2000], Loss: 0.0000\n",
      "  - Epoch [1000/2000], Loss: 0.0000\n",
      "  - Epoch [1020/2000], Loss: 0.0000\n",
      "  - Epoch [1040/2000], Loss: 0.0000\n",
      "  - Epoch [1060/2000], Loss: 0.0000\n",
      "  - Epoch [1080/2000], Loss: 0.0001\n",
      "  - Epoch [1100/2000], Loss: 0.0000\n",
      "  - Epoch [1120/2000], Loss: 0.0000\n",
      "  - Epoch [1140/2000], Loss: 0.0000\n",
      "  - Epoch [1160/2000], Loss: 0.0000\n",
      "  - Epoch [1180/2000], Loss: 0.0000\n",
      "  - Epoch [1200/2000], Loss: 0.0000\n",
      "  - Epoch [1220/2000], Loss: 0.0000\n",
      "  - Epoch [1240/2000], Loss: 0.0000\n",
      "  - Epoch [1260/2000], Loss: 0.0000\n",
      "  - Epoch [1280/2000], Loss: 0.0000\n",
      "  - Epoch [1300/2000], Loss: 0.0000\n",
      "  - Epoch [1320/2000], Loss: 0.0000\n",
      "  - Epoch [1340/2000], Loss: 0.0000\n",
      "  - Epoch [1360/2000], Loss: 0.0000\n",
      "  - Epoch [1380/2000], Loss: 0.0000\n",
      "  - Epoch [1400/2000], Loss: 0.0000\n",
      "  - Epoch [1420/2000], Loss: 0.0000\n",
      "  - Epoch [1440/2000], Loss: 0.0004\n",
      "  - Epoch [1460/2000], Loss: 0.0000\n",
      "  - Epoch [1480/2000], Loss: 0.0000\n",
      "  - Epoch [1500/2000], Loss: 0.0000\n",
      "  - Epoch [1520/2000], Loss: 0.0000\n",
      "  - Epoch [1540/2000], Loss: 0.0000\n",
      "  - Epoch [1560/2000], Loss: 0.0000\n",
      "  - Epoch [1580/2000], Loss: 0.0000\n",
      "  - Epoch [1600/2000], Loss: 0.0000\n",
      "  - Epoch [1620/2000], Loss: 0.0000\n",
      "  - Epoch [1640/2000], Loss: 0.0000\n",
      "  - Epoch [1660/2000], Loss: 0.0000\n",
      "  - Epoch [1680/2000], Loss: 0.0000\n",
      "  - Epoch [1700/2000], Loss: 0.0000\n",
      "  - Epoch [1720/2000], Loss: 0.0000\n",
      "  - Epoch [1740/2000], Loss: 0.0000\n",
      "  - Epoch [1760/2000], Loss: 0.0000\n",
      "  - Epoch [1780/2000], Loss: 0.0000\n",
      "  - Epoch [1800/2000], Loss: 0.0000\n",
      "  - Epoch [1820/2000], Loss: 0.0000\n",
      "  - Epoch [1840/2000], Loss: 0.0000\n",
      "  - Epoch [1860/2000], Loss: 0.0000\n",
      "  - Epoch [1880/2000], Loss: 0.0000\n",
      "  - Epoch [1900/2000], Loss: 0.0000\n",
      "  - Epoch [1920/2000], Loss: 0.0000\n",
      "  - Epoch [1940/2000], Loss: 0.0000\n",
      "  - Epoch [1960/2000], Loss: 0.0000\n",
      "  - Epoch [1980/2000], Loss: 0.0000\n",
      "  - Epoch [2000/2000], Loss: 0.0000\n",
      " **Training complete.**\n",
      "  - Final training loss: 0.0000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # --- 4. Training Setup and Loop ---\n",
    "\n",
    "# **Hyperparameter:** EPOCHS - Number of complete passes over the training data.\n",
    "EPOCHS = 2000\n",
    "# Convert and move data to the selected device (CPU/GPU)\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device) \n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# **Loss Function (Criterion):** nn.MSELoss (Mean Squared Error)\n",
    "# Suitable for regression tasks where we predict a continuous score.\n",
    "criterion = nn.MSELoss()\n",
    "# **Optimizer:** optim.Adam (Adaptive Moment Estimation)\n",
    "# **Hyperparameter:** lr (Learning Rate) - Controls the step size of weight updates.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training Loop\n",
    "loss_history = []\n",
    "print(f\" **Starting training for {EPOCHS} epochs on {device}...**\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    optimizer.zero_grad() # Clear gradients from previous iteration\n",
    "\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    \n",
    "    loss.backward() # Compute gradient of the loss\n",
    "    optimizer.step() # Update model weights\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"  - Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\" **Training complete.**\")\n",
    "print(f\"  - Final training loss: {loss_history[-1]:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba2c964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **Inference Predictions (Score range: -1.0 to 1.0):**\n",
      "\n",
      "  Message: 'The like how things are going perfectlyly in the company and I am thrilled'\n",
      "  Predicted Sentiment: 0.9978 (Expected: Positive)\n",
      "\n",
      "  Message: 'I am not upset with how my device is getting restarted and not charging fast!'\n",
      "  Predicted Sentiment: -0.9148 (Expected: Negative)\n",
      "\n",
      "  Message: 'The sky is blue and the night is fresh.'\n",
      "  Predicted Sentiment: -0.1030 (Expected: Neutral/Zero)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # --- 5. Inference/Testing ---\n",
    "model.eval() # Set model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# Example texts\n",
    "new_text_pos = \"The like how things are going perfectlyly in the company and I am thrilled\"\n",
    "new_text_neg = \"I am not upset with how my device is getting restarted and not charging fast!\"\n",
    "new_text_neu = \"The sky is blue and the night is fresh.\"\n",
    "\n",
    "\n",
    "def predict_sentiment(text, vectorizer, model, device):\n",
    "    \"\"\"Helper function to preprocess and predict sentiment.\"\"\"\n",
    "    # The new text must be transformed using the *same* fitted vectorizer.\n",
    "    new_vec = vectorizer.transform([text]).toarray()\n",
    "    new_tensor = torch.FloatTensor(new_vec).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        prediction = model(new_tensor).item()\n",
    "    return prediction\n",
    "\n",
    "# Get and print predictions\n",
    "prediction_pos = predict_sentiment(new_text_pos, vectorizer, model, device)\n",
    "prediction_neg = predict_sentiment(new_text_neg, vectorizer, model, device)\n",
    "prediction_neu = predict_sentiment(new_text_neu, vectorizer, model, device)\n",
    "\n",
    "print(\" **Inference Predictions (Score range: -1.0 to 1.0):**\")\n",
    "\n",
    "print(f\"\\n  Message: '{new_text_pos}'\")\n",
    "print(f\"  Predicted Sentiment: {prediction_pos:.4f} (Expected: Positive)\")\n",
    "\n",
    "print(f\"\\n  Message: '{new_text_neg}'\")\n",
    "print(f\"  Predicted Sentiment: {prediction_neg:.4f} (Expected: Negative)\")\n",
    "\n",
    "print(f\"\\n  Message: '{new_text_neu}'\")\n",
    "print(f\"  Predicted Sentiment: {prediction_neu:.4f} (Expected: Neutral/Zero)\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d216385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-pytorch-env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
