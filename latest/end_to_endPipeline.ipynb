{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff292bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e3efd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 0. Setup: A Toy Model and Helper Functions ---\n",
    "\n",
    "# A very simple language model for demonstration purposes\n",
    "class ToyLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # We only care about the output of the last token for next-word prediction\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a3597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to generate text from the model\n",
    "def generate_text(model, tokenizer, seed_text, max_len=10):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(seed_text)\n",
    "    input_tensor = torch.tensor([tokens]).to(next(model.parameters()).device)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            # Get the predicted next token (the one with the highest probability)\n",
    "            next_token = output.argmax(1).item()\n",
    "            tokens.append(next_token)\n",
    "            input_tensor = torch.tensor([tokens]).to(next(model.parameters()).device)\n",
    "            \n",
    "            if tokenizer.decode([next_token]) == '<end>':\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5820a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple tokenizer for our toy vocabulary\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        # Flatten the corpus and find unique words\n",
    "        words = sorted(list(set(word for sentence in corpus for word in sentence.split())))\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return [self.word_to_idx[word] for word in text.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.idx_to_word[token] for token in tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e80275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Phase 1: Foundational Pre-training ---\n",
    "def simulate_pretraining(model, tokenizer, corpus, epochs=50):\n",
    "    print(\"\\n--- Starting Phase 1: Foundational Pre-training ---\")\n",
    "    print(\"Goal: Teach the model basic language structure by predicting the next word.\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create input/output pairs for next-word prediction\n",
    "    inputs, targets = [], []\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer.encode(sentence)\n",
    "        for i in range(1, len(tokens)):\n",
    "            inputs.append(tokens[:i])\n",
    "            targets.append(tokens[i])\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_seq = torch.tensor([inputs[i]])\n",
    "            target_val = torch.tensor([targets[i]])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target_val)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Pre-training Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(inputs):.4f}\")\n",
    "\n",
    "    print(\"--- Pre-training Complete ---\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1637f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Phase 2: Supervised Fine-Tuning (SFT) ---\n",
    "def simulate_sft(model, tokenizer, sft_data, epochs=30):\n",
    "    print(\"\\n--- Starting Phase 2: Supervised Fine-Tuning (SFT) ---\")\n",
    "    print(\"Goal: Teach the model to follow instructions in a question-answer format.\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for prompt, ideal_response in sft_data.items():\n",
    "            # The input is the prompt, and the target is the ideal response\n",
    "            input_tokens = tokenizer.encode(prompt)\n",
    "            target_tokens = tokenizer.encode(ideal_response)\n",
    "            \n",
    "            # We'll train the model to generate the response one word at a time\n",
    "            for i in range(len(target_tokens)):\n",
    "                current_input = torch.tensor([input_tokens + target_tokens[:i]])\n",
    "                current_target = torch.tensor([target_tokens[i]])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(current_input)\n",
    "                loss = criterion(output, current_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  SFT Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(sft_data):.4f}\")\n",
    "            \n",
    "    print(\"--- SFT Complete ---\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3f4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Phase 3: Reinforcement Learning with Human Feedback (RLHF) ---\n",
    "\n",
    "# Step 3a: Simulate a Reward Model\n",
    "def get_reward_score(response):\n",
    "    \"\"\"\n",
    "    A simple, rule-based reward model. In reality, this would be a separate,\n",
    "    trained neural network.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    if \"i can help\" in response: score += 1.5 # Prefers helpfulness\n",
    "    if \"of course\" in response: score += 1.0\n",
    "    if \"sun is hot\" in response: score += 1.0 # Prefers factual correctness\n",
    "    if \"sky is blue\" in response: score += 1.0\n",
    "    if \"i am a bot\" in response: score -= 2.0 # Penalizes unhelpful or robotic answers\n",
    "    if len(response.split()) < 5: score -= 1.0 # Penalizes short, uninformative answers\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb65c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3b: Fine-tune with Reinforcement Learning\n",
    "def simulate_rlhf(model, tokenizer, prompts, iterations=50):\n",
    "    print(\"\\n--- Starting Phase 3: Reinforcement Learning with Human Feedback (RLHF) ---\")\n",
    "    print(\"Goal: Refine the model based on preferences (what makes a 'good' answer).\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Pick a random prompt and generate a response\n",
    "        prompt = random.choice(prompts)\n",
    "        response_str = generate_text(model, tokenizer, prompt, max_len=15)\n",
    "        \n",
    "        # Get a score from our simulated reward model\n",
    "        reward = get_reward_score(response_str)\n",
    "        \n",
    "        # This is a simplified version of a policy gradient update (like PPO).\n",
    "        # We calculate a \"loss\" that is inversely proportional to the reward.\n",
    "        # A high reward means a low loss, and a low reward means a high loss.\n",
    "        # This encourages the model to generate responses that get high rewards.\n",
    "        loss = -reward \n",
    "        \n",
    "        # We need to backpropagate this \"loss\" through the generation process.\n",
    "        # This is complex, so we'll simulate it by treating the reward as a loss\n",
    "        # on the log probabilities of the generated tokens.\n",
    "        \n",
    "        # A simplified pseudo-loss calculation\n",
    "        log_probs = []\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "        response_tokens = tokenizer.encode(response_str.replace(prompt, '').strip())\n",
    "        \n",
    "        for token in response_tokens:\n",
    "            input_tensor = torch.tensor([input_tokens])\n",
    "            output = model(input_tensor)\n",
    "            log_prob = torch.log_softmax(output, dim=1)[0, token]\n",
    "            log_probs.append(log_prob)\n",
    "            input_tokens.append(token)\n",
    "            \n",
    "        if log_probs:\n",
    "            policy_loss = -torch.stack(log_probs).mean() * reward\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  RLHF Iteration {i+1}/{iterations}, Prompt: '{prompt}', Reward: {reward:.2f}\")\n",
    "\n",
    "    print(\"--- RLHF Complete ---\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a196d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Untrained Model ---\n",
      "Prompt: 'the sun is' -> Response: 'the sun is sun of of you sun of you sun of you'\n",
      "\n",
      "--- Starting Phase 1: Foundational Pre-training ---\n",
      "Goal: Teach the model basic language structure by predicting the next word.\n",
      "  Pre-training Epoch 10/50, Loss: 0.2506\n",
      "  Pre-training Epoch 20/50, Loss: 0.0949\n",
      "  Pre-training Epoch 30/50, Loss: 0.0771\n",
      "  Pre-training Epoch 40/50, Loss: 0.0709\n",
      "  Pre-training Epoch 50/50, Loss: 0.0677\n",
      "--- Pre-training Complete ---\n",
      "\n",
      "--- Model after Pre-training ---\n",
      "Prompt: 'the sun is' -> Response: 'the sun is hot <end>'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'me'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Model after Pre-training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe sun is\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -> Response: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_text(base_model,\u001b[38;5;250m \u001b[39mtokenizer,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe sun is\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan you help me\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -> Response: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcan you help me\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Phase 2\u001b[39;00m\n\u001b[0;32m     40\u001b[0m sft_model \u001b[38;5;241m=\u001b[39m simulate_sft(base_model, tokenizer, sft_dataset)\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, tokenizer, seed_text, max_len)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(model, tokenizer, seed_text, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([tokens])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'me'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution ---\n",
    "#if __name__ == \"__main__\":\n",
    "    # --- Data and Vocabulary Setup ---\n",
    "pretrain_corpus = [\n",
    "        \"the sun is hot <end>\",\n",
    "        \"the sky is blue <end>\",\n",
    "        \"i like to code <end>\",\n",
    "        \"what is your name <end>\",\n",
    "        \"my name is bot <end>\",\n",
    "        \"how can i help you <end>\",\n",
    "    ]\n",
    "    \n",
    "    # Add a special <end> token to our vocabulary\n",
    "full_corpus = pretrain_corpus + [\"<end>\", \"of course i can help\", \"i am a bot\"]\n",
    "tokenizer = SimpleTokenizer(full_corpus)\n",
    "vocab_size = len(tokenizer.word_to_idx)\n",
    "    \n",
    "sft_dataset = {\n",
    "        \"what is the sun\": \"the sun is hot <end>\",\n",
    "        \"what color is the sky\": \"the sky is blue <end>\",\n",
    "        \"can you help me\": \"of course i can help <end>\"\n",
    "    }\n",
    "    \n",
    "rlhf_prompts = [\"can you help me\", \"what is your name\", \"what is the sun\"]\n",
    "\n",
    "    # --- Initialize the Model ---\n",
    "toy_model = ToyLLM(vocab_size=vocab_size, embed_dim=10, hidden_dim=20)\n",
    "\n",
    "    # --- Run the Pipeline ---\n",
    "print(\"--- Initial Untrained Model ---\")\n",
    "print(f\"Prompt: 'the sun is' -> Response: '{generate_text(toy_model, tokenizer, 'the sun is')}'\")\n",
    "    \n",
    "    # Phase 1\n",
    "base_model = simulate_pretraining(toy_model, tokenizer, pretrain_corpus)\n",
    "print(\"\\n--- Model after Pre-training ---\")\n",
    "print(f\"Prompt: 'the sun is' -> Response: '{generate_text(base_model, tokenizer, 'the sun is')}'\")\n",
    "print(f\"Prompt: 'can you help me' -> Response: '{generate_text(base_model, tokenizer, 'can you help me')}'\")\n",
    "\n",
    "    # Phase 2\n",
    "sft_model = simulate_sft(base_model, tokenizer, sft_dataset)\n",
    "print(\"\\n--- Model after SFT ---\")\n",
    "print(f\"Prompt: 'what is the sun' -> Response: '{generate_text(sft_model, tokenizer, 'what is the sun')}'\")\n",
    "print(f\"Prompt: 'can you help me' -> Response: '{generate_text(sft_model, tokenizer, 'can you help me')}'\")\n",
    "print(f\"Prompt: 'what is your name' -> Response: '{generate_text(sft_model, tokenizer, 'what is your name')}'\") # May still give a poor answer\n",
    "\n",
    "    # Phase 3\n",
    "rlhf_model = simulate_rlhf(sft_model, tokenizer, rlhf_prompts)\n",
    "print(\"\\n--- Final Model after RLHF ---\")\n",
    "print(\"Note how the model now prefers the more 'helpful' sounding response.\")\n",
    "print(f\"Prompt: 'what is the sun' -> Response: '{generate_text(rlhf_model, tokenizer, 'what is the sun')}'\")\n",
    "print(f\"Prompt: 'can you help me' -> Response: '{generate_text(rlhf_model, tokenizer, 'can you help me')}'\")\n",
    "print(f\"Prompt: 'what is your name' -> Response: '{generate_text(rlhf_model, tokenizer, 'what is your name')}'\") #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-pytorch-env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
