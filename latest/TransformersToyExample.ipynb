{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7f7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 44\n",
      "Sequence Length (context window): 4\n",
      "Total training examples: 53\n",
      "Starting pre-training with 3000 epochs...\n",
      "Epoch [500/3000], Loss: 0.1051\n",
      "Epoch [1000/3000], Loss: 0.0786\n",
      "Epoch [1500/3000], Loss: 0.0785\n",
      "Epoch [2000/3000], Loss: 0.0785\n",
      "Epoch [2500/3000], Loss: 0.0785\n",
      "Epoch [3000/3000], Loss: 0.0785\n",
      "Pre-training complete.\n",
      "\n",
      "--- Generating Text (Max 20 tokens) ---\n",
      "Start: 'alice was beginning to get'\n",
      "\n",
      "Resulting Text:\n",
      "alice was beginning to get very tired of sitting nothing to do: once or twice she had peeped into the book her sister on\n",
      "\n",
      "--- ATTENTION MECHANISM SUMMARY ---\n",
      "In the SelfAttention class, the key operation is:\n",
      "1. Q, K, V Projections: Maps input vector (x) into three roles.\n",
      "2. Scoring: torch.matmul(Q, K.transpose) calculates pairwise similarity.\n",
      "3. Weighting: Softmax turns these scores into probabilistic attention weights.\n",
      "4. Context: torch.matmul(Weights, V) sums the V vectors based on the computed weights.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# --- Data Preparation ---\n",
    "\n",
    "# A simple, small corpus from Alice in Wonderland\n",
    "corpus = \"\"\"\n",
    "alice was beginning to get very tired of sitting by her sister on the bank,\n",
    "and of having nothing to do: once or twice she had peeped into the book her\n",
    "sister was reading, but it had no pictures or conversations in it, and what\n",
    "is the use of a book, thought alice without pictures or conversations?\n",
    "\"\"\"\n",
    "\n",
    "# Tokenization and Vocabulary\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, text):\n",
    "        # Convert all text to lower case and split by whitespace\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        # Create a set of unique words\n",
    "        vocab = sorted(list(set(tokens)))\n",
    "        \n",
    "        # Map words to unique integers (token IDs)\n",
    "        self.word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab) + 1 # +1 for padding/unknown token (index 0)\n",
    "\n",
    "    def encode(self, text, max_len=None):\n",
    "        tokens = text.lower().split()\n",
    "        encoded = [self.word_to_idx.get(word, 0) for word in tokens]\n",
    "        \n",
    "        if max_len:\n",
    "            # Pad or truncate the sequence\n",
    "            if len(encoded) < max_len:\n",
    "                encoded += [0] * (max_len - len(encoded))\n",
    "            else:\n",
    "                encoded = encoded[:max_len]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        # Decode only if the index is not 0 (padding/unknown)\n",
    "        return ' '.join([self.idx_to_word[idx] for idx in encoded if idx != 0 and idx in self.idx_to_word])\n",
    "\n",
    "# Create sequences for next-word prediction\n",
    "def create_sequences(tokenizer, corpus, seq_len=4):\n",
    "    tokens = corpus.lower().split()\n",
    "    encoded_tokens = tokenizer.encode(corpus)\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(encoded_tokens) - seq_len):\n",
    "        # Input: sequence of length seq_len\n",
    "        input_seq = encoded_tokens[i:i + seq_len]\n",
    "        # Target: the token immediately following the input sequence\n",
    "        target_token = encoded_tokens[i + seq_len]\n",
    "        \n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_token)\n",
    "        \n",
    "    return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "# --- Transformer Core Component: Scaled Dot-Product Self-Attention ---\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified single-head Self-Attention mechanism, the heart of the Transformer.\n",
    "    This replaces the sequential processing of an RNN/LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear layers to project the input embedding into Query, Key, and Value vectors\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # 1. Project to Q, K, V\n",
    "        Q = self.W_q(x) # Query: What am I looking for?\n",
    "        K = self.W_k(x) # Key: What do I have?\n",
    "        V = self.W_v(x) # Value: What context should I pass?\n",
    "        \n",
    "        # 2. Compute Attention Scores (Scaled Dot-Product)\n",
    "        # Q @ K.transpose(-2, -1) calculates the similarity between all pairs of words.\n",
    "        # Scaling by sqrt(d_k) stabilizes the gradient.\n",
    "        d_k = Q.size(-1)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # 3. Normalize Scores\n",
    "        # Softmax turns scores into probability weights (0 to 1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 4. Apply Weights to Values\n",
    "        # The output is a weighted sum of the Value vectors (Context Vector)\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return context_vector # shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "\n",
    "# --- The Toy LLM using Attention ---\n",
    "\n",
    "class ToyAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal LLM architecture using an embedding layer followed by Self-Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 1. Embedding Layer: Converts token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 2. Positional Encoding (Crucial for Transformers)\n",
    "        # Since attention processes all words at once, we must inject their order.\n",
    "        self.positional_encoding = nn.Embedding(seq_len, embed_dim)\n",
    "\n",
    "        # 3. Self-Attention Block (The new core)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "\n",
    "        # 4. Final Linear Layer: Maps the context vector back to the vocab size\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # 1. Look up word embeddings\n",
    "        word_embeddings = self.embedding(x) # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # 2. Add Positional Encoding\n",
    "        # Generate position indices (0, 1, 2, 3...)\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embeddings = self.positional_encoding(positions)\n",
    "        \n",
    "        # Transformer Input = Word Embedding + Positional Encoding\n",
    "        x = word_embeddings + pos_embeddings\n",
    "        \n",
    "        # 3. Pass through Self-Attention\n",
    "        context_vector = self.attention(x) # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # For next-word prediction, we only care about the last word's context\n",
    "        # to predict the next word in the sequence.\n",
    "        last_context = context_vector[:, -1, :] # (batch_size, embed_dim)\n",
    "\n",
    "        # 4. Output: Predict the next token\n",
    "        output = self.fc(last_context) # (batch_size, vocab_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# --- Training and Inference Functions ---\n",
    "\n",
    "def simulate_pretraining(model, inputs, targets, epochs=3000): # Increased epochs\n",
    "    \"\"\"\n",
    "    Simulates the foundational pre-training phase (Next-Word Prediction).\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    print(f\"Starting pre-training with {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        # 1. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2. Calculate Loss (Error)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 3. Backpropagation (Find Blame)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Optimization (Adjust Weights)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    print(\"Pre-training complete.\")\n",
    "    return model\n",
    "\n",
    "def generate_text(model, tokenizer, start_phrase, seq_len=4, max_tokens=10, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generates text using the trained model, using temperature sampling\n",
    "    to prevent getting stuck in repetitive loops.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    generated_tokens = tokenizer.encode(start_phrase, max_len=seq_len)\n",
    "    \n",
    "    if len(generated_tokens) < seq_len:\n",
    "        print(\"Error: Start phrase is too short for the required sequence length.\")\n",
    "        return start_phrase\n",
    "\n",
    "    print(f\"\\n--- Generating Text (Max {max_tokens} tokens) ---\")\n",
    "    print(f\"Start: '{start_phrase}'\")\n",
    "    \n",
    "    output_tokens = generated_tokens\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculations during inference\n",
    "        for _ in range(max_tokens):\n",
    "            # 1. Prepare the current sequence input\n",
    "            current_sequence = torch.tensor([output_tokens[-seq_len:]])\n",
    "            \n",
    "            # 2. Get prediction from the Attention Model\n",
    "            output = model(current_sequence) # (1, vocab_size)\n",
    "            \n",
    "            # 3. Apply Temperature (dividing logits by T)\n",
    "            output = output / temperature\n",
    "            \n",
    "            # 4. Convert to probabilities and sample (instead of argmax)\n",
    "            probabilities = F.softmax(output, dim=-1)\n",
    "            predicted_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            \n",
    "            if predicted_token_id == 0: # Stop on padding/unknown\n",
    "                break\n",
    "            \n",
    "            # 5. Add the new token to the sequence for the next step\n",
    "            output_tokens.append(predicted_token_id)\n",
    "            \n",
    "            # Stop if the model starts repeating itself or generating gibberish\n",
    "            if len(output_tokens) > 2 * seq_len and len(set(output_tokens[-seq_len:])) < 2:\n",
    "                 break\n",
    "\n",
    "    # Decode and return the result\n",
    "    return tokenizer.decode(output_tokens)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define hyper-parameters\n",
    "    EMBED_DIM = 32    # Size of the word vector (Increased from 16)\n",
    "    SEQ_LEN = 4       # Number of words the model looks at to predict the next\n",
    "    EPOCHS = 3000     # Number of training iterations (Increased from 1500)\n",
    "\n",
    "    # 1. Data Setup\n",
    "    tokenizer = SimpleTokenizer(corpus)\n",
    "    VOCAB_SIZE = tokenizer.vocab_size\n",
    "    \n",
    "    inputs, targets = create_sequences(tokenizer, corpus, seq_len=SEQ_LEN)\n",
    "    \n",
    "    print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
    "    print(f\"Sequence Length (context window): {SEQ_LEN}\")\n",
    "    print(f\"Total training examples: {len(inputs)}\")\n",
    "\n",
    "    # 2. Model Initialization (Using the new Attention Model)\n",
    "    model = ToyAttentionModel(VOCAB_SIZE, EMBED_DIM, SEQ_LEN)\n",
    "    \n",
    "    # 3. Training\n",
    "    model = simulate_pretraining(model, inputs, targets, epochs=EPOCHS)\n",
    "    \n",
    "    # 4. Inference (Generate text)\n",
    "    start_phrase = \"alice was beginning to get\"\n",
    "    generated_text = generate_text(model, tokenizer, start_phrase, seq_len=SEQ_LEN, max_tokens=20)\n",
    "    \n",
    "    print(f\"\\nResulting Text:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Simple summary of the attention core\n",
    "    print(\"\\n--- ATTENTION MECHANISM SUMMARY ---\")\n",
    "    print(\"In the SelfAttention class, the key operation is:\")\n",
    "    print(\"1. Q, K, V Projections: Maps input vector (x) into three roles.\")\n",
    "    print(\"2. Scoring: torch.matmul(Q, K.transpose) calculates pairwise similarity.\")\n",
    "    print(\"3. Weighting: Softmax turns these scores into probabilistic attention weights.\")\n",
    "    print(\"4. Context: torch.matmul(Weights, V) sums the V vectors based on the computed weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8740c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### --- CELL 1: SETUP & HYPERPARAMETERS --- ###\n",
      "EMBED_DIM: 32, SEQ_LEN: 4, EPOCHS: 3000, TEMP: 0.8\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "### --- CELL 1: SETUP & HYPERPARAMETERS --- ###\n",
    "print(\"### --- CELL 1: SETUP & HYPERPARAMETERS --- ###\")\n",
    "\n",
    "# Define hyper-parameters for the model\n",
    "EMBED_DIM = 32      # Size of the word vector\n",
    "SEQ_LEN = 4         # Context window size (e.g., predict word 5 from words 1-4)\n",
    "EPOCHS = 3000       # Number of training iterations\n",
    "TEMPERATURE = 0.8   # Sampling temperature (for randomness in generation)\n",
    "\n",
    "print(f\"EMBED_DIM: {EMBED_DIM}, SEQ_LEN: {SEQ_LEN}, EPOCHS: {EPOCHS}, TEMP: {TEMPERATURE}\")\n",
    "\n",
    "# Placeholder for the device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6738b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### --- CELL 2: DATA UTILITIES (TOKENIZER & SEQUENCE CREATION) --- ###\n",
      "Tokenizer and sequence creation utilities defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### --- CELL 2: DATA UTILITIES (TOKENIZER & SEQUENCE CREATION) --- ###\n",
    "print(\"\\n### --- CELL 2: DATA UTILITIES (TOKENIZER & SEQUENCE CREATION) --- ###\")\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Handles tokenization and mapping words to integers (IDs).\"\"\"\n",
    "    def __init__(self, text):\n",
    "        tokens = text.lower().split()\n",
    "        vocab = sorted(list(set(tokens)))\n",
    "        \n",
    "        # Word-to-index mapping (index 0 is reserved for padding/unknown)\n",
    "        self.word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab) + 1 \n",
    "\n",
    "    def encode(self, text, max_len=None):\n",
    "        tokens = text.lower().split()\n",
    "        encoded = [self.word_to_idx.get(word, 0) for word in tokens]\n",
    "        \n",
    "        if max_len:\n",
    "            if len(encoded) < max_len:\n",
    "                encoded += [0] * (max_len - len(encoded))\n",
    "            else:\n",
    "                encoded = encoded[:max_len]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        return ' '.join([self.idx_to_word[idx] for idx in encoded if idx != 0 and idx in self.idx_to_word])\n",
    "\n",
    "def create_sequences(encoded_tokens, seq_len=4):\n",
    "    \"\"\"Generates input (X) and target (Y) pairs for training.\"\"\"\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(encoded_tokens) - seq_len):\n",
    "        input_seq = encoded_tokens[i:i + seq_len]\n",
    "        target_token = encoded_tokens[i + seq_len]\n",
    "        \n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_token)\n",
    "        \n",
    "    return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "print(\"Tokenizer and sequence creation utilities defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b73b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### --- CELL 3: MODEL COMPONENTS (ATTENTION & TOY LLM) --- ###\n",
      "Attention model components defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### --- CELL 3: MODEL COMPONENTS (ATTENTION & TOY LLM) --- ###\n",
    "print(\"\\n### --- CELL 3: MODEL COMPONENTS (ATTENTION & TOY LLM) --- ###\")\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Self-Attention mechanism.\"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Q, K, V projection matrices\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Compute Attention Scores: Q @ K.transpose / sqrt(d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Apply Softmax to get probability weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply weights to Values to get the context vector\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return context_vector\n",
    "\n",
    "class ToyAttentionModel(nn.Module):\n",
    "    \"\"\"Minimal LLM combining Embedding, Positional Encoding, and Self-Attention.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Embedding(seq_len, embed_dim)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # 1. Word and Positional Embeddings\n",
    "        word_embeddings = self.embedding(x)\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embeddings = self.positional_encoding(positions)\n",
    "        x = word_embeddings + pos_embeddings\n",
    "        \n",
    "        # 2. Self-Attention\n",
    "        context_vector = self.attention(x)\n",
    "        \n",
    "        # 3. Predict the next token based on the context of the last word\n",
    "        last_context = context_vector[:, -1, :] \n",
    "\n",
    "        # 4. Final classification layer\n",
    "        output = self.fc(last_context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"Attention model components defined.\")\n",
    "\n",
    "\n",
    "def simulate_pretraining(model, inputs, targets, epochs, device):\n",
    "    \"\"\"Simulates the training loop.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"Starting pre-training with {epochs} epochs...\")\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    print(\"Pre-training complete.\")\n",
    "    return model\n",
    "\n",
    "def generate_text(model, tokenizer, start_phrase, seq_len, max_tokens, temperature, device):\n",
    "    \"\"\"Generates text using temperature sampling.\"\"\"\n",
    "    model.eval()\n",
    "    generated_tokens = tokenizer.encode(start_phrase, max_len=seq_len)\n",
    "    output_tokens = generated_tokens\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            # Use the last SEQ_LEN tokens as input\n",
    "            current_sequence = torch.tensor([output_tokens[-seq_len:]]).to(device)\n",
    "            \n",
    "            output = model(current_sequence)\n",
    "            \n",
    "            # Apply Temperature and Softmax\n",
    "            output = output / temperature\n",
    "            probabilities = F.softmax(output, dim=-1)\n",
    "            \n",
    "            # Sample the next token\n",
    "            predicted_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            \n",
    "            if predicted_token_id == 0: # Stop on padding/unknown\n",
    "                break\n",
    "            \n",
    "            output_tokens.append(predicted_token_id)\n",
    "            \n",
    "            # Simple check to prevent repetitive loops\n",
    "            if len(output_tokens) > 2 * seq_len and len(set(output_tokens[-seq_len:])) < 2:\n",
    "                 break\n",
    "\n",
    "    return tokenizer.decode(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a039192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "### --- CELL 4: SIMULATED PDF TEXT EXTRACTION --- ###\n",
      "\n",
      "  Downloading pypdf-6.1.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-6.1.3-py3-none-any.whl (323 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded (simulated PDF text).\n",
      "Corpus size (tokens): 51743\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "### --- CELL 4: SIMULATED PDF TEXT EXTRACTION --- ###\n",
    "print(\"\\n### --- CELL 4: SIMULATED PDF TEXT EXTRACTION --- ###\")\n",
    "    \n",
    "    # NOTE: In a real Jupyter environment, you would use a library like pypdf \n",
    "    # to extract text from a file path:\n",
    "    # \n",
    "import pypdf\n",
    "pdf_path = \"the-prince.pdf\"\n",
    "reader = pypdf.PdfReader(pdf_path)\n",
    "text_content = \"\".join(page.extract_text() for page in reader.pages)\n",
    "    \n",
    "    # For this simulation, we use a large sample text representing the extracted content:\n",
    "#    text_content = \"\"\"\n",
    "#    the quick brown fox jumps over the lazy dog the dog was sleeping\n",
    "#    and the fox was quick and brown the quick fox needed to jump to\n",
    "#    get food because the dog was very lazy\n",
    "#    \"\"\"\n",
    "    \n",
    "    # Clean and flatten the text\n",
    "corpus = \" \".join(text_content.lower().split())\n",
    "\n",
    "print(\"Corpus loaded (simulated PDF text).\")\n",
    "print(f\"Corpus size (tokens): {len(corpus.split())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4263b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### --- CELL 5: DATA PREPARATION & MODEL INITIALIZATION --- ###\n",
      "Vocabulary Size: 8283\n",
      "Total training examples: 51739\n",
      "Sample Input (IDs): [2339, 3209, 2427, 5102]\n",
      "Sample Target (ID): 1450\n",
      "\n",
      "Model instantiated and moved to device.\n"
     ]
    }
   ],
   "source": [
    "### --- CELL 5: DATA PREPARATION & MODEL INITIALIZATION --- ###\n",
    "print(\"\\n### --- CELL 5: DATA PREPARATION & MODEL INITIALIZATION --- ###\")\n",
    "    \n",
    "    # 1. Tokenization\n",
    "tokenizer = SimpleTokenizer(corpus)\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "    \n",
    "    # 2. Sequence Creation\n",
    "encoded_tokens = tokenizer.encode(corpus)\n",
    "inputs, targets = create_sequences(encoded_tokens, seq_len=SEQ_LEN)\n",
    "    \n",
    "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
    "print(f\"Total training examples: {len(inputs)}\")\n",
    "print(f\"Sample Input (IDs): {inputs[0].tolist()}\")\n",
    "print(f\"Sample Target (ID): {targets[0].item()}\")\n",
    "\n",
    "    # 3. Model Initialization\n",
    "model = ToyAttentionModel(VOCAB_SIZE, EMBED_DIM, SEQ_LEN).to(DEVICE)\n",
    "print(\"\\nModel instantiated and moved to device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecf471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### --- CELL 6: TRAINING THE MODEL --- ###\n",
      "Starting pre-training with 100 epochs...\n"
     ]
    }
   ],
   "source": [
    "### --- CELL 6: TRAINING THE MODEL --- ###\n",
    "print(\"\\n### --- CELL 6: TRAINING THE MODEL --- ###\")\n",
    "EPOCHS = 100    \n",
    "# Train the model\n",
    "model = simulate_pretraining(model, inputs, targets, epochs=EPOCHS, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ae173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- CELL 7: INFERENCE AND GENERATION --- ###\n",
    "print(\"\\n### --- CELL 7: INFERENCE AND GENERATION --- ###\")\n",
    "    \n",
    "    # Define the starting phrase (must match words in the corpus)\n",
    "start_phrase = \"live in freedom\" \n",
    " \n",
    "print(f\"Starting phrase: '{start_phrase}'\")\n",
    "\n",
    "    # Generate the text\n",
    "generated_text = generate_text(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    start_phrase, \n",
    "    seq_len=SEQ_LEN, \n",
    "    max_tokens=15, \n",
    "    temperature=TEMPERATURE, \n",
    "    device=DEVICE\n",
    ")\n",
    "    \n",
    "print(\"\\n--- GENERATED TEXT ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- CELL 8: SUPERVISED FINE-TUNING (SFT) --- ###\n",
    "    print(\"\\n--- --- --- CELL 8: SUPERVISED FINE-TUNING (SFT) --- --- ---\")\n",
    "    \n",
    "    # 1. Prepare SFT Data (Instruction + Desired Response)\n",
    "    sft_data = [\n",
    "        (\"Tell me about the fox\", f\"{SEP_TOKEN} the fox was quick and brown and needed to jump\"),\n",
    "        (\"Why did the fox jump\", f\"{SEP_TOKEN} to get food because the dog was lazy\"),\n",
    "        (\"Describe the dog\", f\"{SEP_TOKEN} the lazy dog was sleeping\")\n",
    "    ]\n",
    "    \n",
    "    # 2. Convert SFT data to sequences\n",
    "    sft_inputs_list, sft_targets_list = [], []\n",
    "    for prompt, completion in sft_data:\n",
    "        full_text = prompt + \" \" + completion # E.g., \"Tell me... : the fox was quick...\"\n",
    "        encoded = tokenizer.encode(full_text)\n",
    "        \n",
    "        # We need to train the model to predict the next token in the instruction-response pair.\n",
    "        sft_inputs_temp, sft_targets_temp = create_sequences(encoded, seq_len=SEQ_LEN)\n",
    "        sft_inputs_list.append(sft_inputs_temp)\n",
    "        sft_targets_list.append(sft_targets_temp)\n",
    "\n",
    "    sft_inputs = torch.cat(sft_inputs_list)\n",
    "    sft_targets = torch.cat(sft_targets_list)\n",
    "    \n",
    "    print(f\"SFT Examples prepared: {len(sft_data)}. Total SFT steps: {len(sft_inputs)}\")\n",
    "    \n",
    "    # 3. Perform SFT (Tune the base model)\n",
    "    sft_model = deepcopy(base_model) # Start SFT from the pre-trained weights\n",
    "    sft_model = train_model(sft_model, sft_inputs, sft_targets, epochs=SFT_EPOCHS, device=DEVICE, title=\"SFT\")\n",
    "\n",
    "    # 4. SFT Inference Test\n",
    "    sft_prompt = \"Why did the fox jump\"\n",
    "    sft_start_tokens = tokenizer.encode(sft_prompt + \" \" + SEP_TOKEN) # Start with instruction + separator\n",
    "    \n",
    "    sft_generation = generate_text(\n",
    "        sft_model, \n",
    "        tokenizer, \n",
    "        sft_start_tokens, \n",
    "        seq_len=SEQ_LEN, \n",
    "        max_tokens=15, \n",
    "        temperature=TEMPERATURE, \n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- SFT Model Test ---\")\n",
    "    print(f\"Instruction: '{sft_prompt}'\")\n",
    "    print(f\"SFT Model Response: {sft_generation}\")\n",
    "\n",
    "\n",
    "    ### --- CELL 9: REINFORCEMENT LEARNING WITH HUMAN FEEDBACK (RLHF) --- ###\n",
    "    print(\"\\n--- --- --- CELL 9: REINFORCEMENT LEARNING WITH HUMAN FEEDBACK (RLHF) --- --- ---\")\n",
    "\n",
    "    # 1. Define a Mock Reward Model (RM)\n",
    "    # Goal: We want the model to favor responses that talk about 'food' more.\n",
    "    def reward_model_score(text):\n",
    "        score = 0\n",
    "        if \"food\" in text:\n",
    "            score += 5.0 # High reward for mentioning the key concept\n",
    "        if \"quick\" in text:\n",
    "            score += 2.0\n",
    "        if \"lazy\" in text:\n",
    "            score -= 1.0 # Penalize for mentioning 'lazy'\n",
    "        return score\n",
    "\n",
    "    print(\"Mock Reward Model defined: rewards responses about 'food'.\")\n",
    "    \n",
    "    # 2. RLHF Loop (Simulated PPO Step)\n",
    "    \n",
    "    # a. Generate a response from the current SFT Policy (Policy = sft_model)\n",
    "    rlhf_prompt = \"Tell me about the fox\"\n",
    "    rlhf_start_tokens = tokenizer.encode(rlhf_prompt + \" \" + SEP_TOKEN)\n",
    "\n",
    "    # Use a higher temperature to explore different generations\n",
    "    generated_response = generate_text(\n",
    "        sft_model, \n",
    "        tokenizer, \n",
    "        rlhf_start_tokens, \n",
    "        seq_len=SEQ_LEN, \n",
    "        max_tokens=15, \n",
    "        temperature=1.2, # Higher temp for diverse exploration\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # b. Score the response using the Reward Model (RM)\n",
    "    reward = reward_model_score(generated_response)\n",
    "    print(f\"\\nInstruction: '{rlhf_prompt}'\")\n",
    "    print(f\"Generated Response: '{generated_response}'\")\n",
    "    print(f\"Reward Model Score (RM): {reward:.2f}\")\n",
    "\n",
    "    # c. Policy Update (Simulated PPO step based on high reward)\n",
    "    # In reality, this would use PPO to adjust the model based on the reward.\n",
    "    # Here, we simulate the update by running a few more targeted SFT steps\n",
    "    # *only* on the highly-rewarded sequence to reinforce it.\n",
    "    \n",
    "    # Mock RL Policy Update:\n",
    "    if reward > 4.5:\n",
    "        print(\"\\nReward is high (>4.5)! Simulating Policy (PPO) Update...\")\n",
    "        \n",
    "        # Create a tiny, focused training set from the rewarded generation\n",
    "        reinforced_text = rlhf_prompt + \" \" + SEP_TOKEN + \" \" + generated_response\n",
    "        reinforced_encoded = tokenizer.encode(reinforced_text)\n",
    "        \n",
    "        rlhf_inputs, rlhf_targets = create_sequences(reinforced_encoded, seq_len=SEQ_LEN)\n",
    "        \n",
    "        # Run a very small, focused training on this single sequence\n",
    "        final_model = train_model(sft_model, rlhf_inputs, rlhf_targets, epochs=10, device=DEVICE, title=\"RLHF Update\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nReward is too low. Policy Update skipped.\")\n",
    "        final_model = sft_model\n",
    "\n",
    "    # 3. Final Inference (RLHF-aligned Model)\n",
    "    print(\"\\n--- RLHF-Aligned Model Test ---\")\n",
    "    \n",
    "    rlhf_final_generation = generate_text(\n",
    "        final_model, \n",
    "        tokenizer, \n",
    "        rlhf_start_tokens, \n",
    "        seq_len=SEQ_LEN, \n",
    "        max_tokens=15, \n",
    "        temperature=TEMPERATURE, \n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    print(f\"Instruction: '{rlhf_prompt}'\")\n",
    "    print(f\"RLHF Model Response: {rlhf_final_generation}\")\n",
    "\n",
    "    # 4. Cleanup\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-pytorch-env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
