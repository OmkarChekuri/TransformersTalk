{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcb9367",
   "metadata": {},
   "source": [
    "Setup: Imports and helper functions.\n",
    "\n",
    "The Data: Defining our \"toy\" language.\n",
    "\n",
    "Concept 1: Tokenization (The Vocabulary): Defining and demoing the SimpleTokenizer.\n",
    "\n",
    "Concept 2: Word Embeddings (The Meaning): Explaining and demoing the nn.Embedding layer.\n",
    "\n",
    "Concept 3: The Neural Network (The \"Brain\"): Building the ToyLLM class.\n",
    "\n",
    "Helper Function: The generate_text function.\n",
    "\n",
    "Initialization: Creating the model and testing its \"untrained\" (random) output.\n",
    "\n",
    "Phase 1: Pre-training (Learning Language): Running simulate_pretraining and testing the result.\n",
    "\n",
    "Phase 2: SFT (Learning to Answer): Running simulate_sft and testing the result.\n",
    "\n",
    "Phase 3: RLHF (Learning \"Preferences\"): Defining the reward model and the simulate_rlhf function.\n",
    "\n",
    "Final Run: Running the RLHF phase and comparing the final model's answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff292bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and random seed set.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup & Imports ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility.\n",
    "# This ensures that every time we run this notebook, the \"random\"\n",
    "# initial weights of our model are the same, leading to the same\n",
    "# training results.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Libraries imported and random seed set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "178c937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-training sentences: 12\n",
      "Total SFT pairs: 6\n",
      "Total RLHF prompts: 5\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: The \"Alice\" Data ---\n",
    "\n",
    "# 1. Pre-training Corpus: Sentences from \"Alice in Wonderland\"\n",
    "#    This teaches the model the world's basic facts and language.\n",
    "pretrain_corpus = [\n",
    "    \"alice followed the white rabbit <end>\",\n",
    "    \"alice went down the rabbit hole <end>\",\n",
    "    \"the white rabbit was late <end>\",\n",
    "    \"the cheshire cat smiled <end>\",\n",
    "    \"the hatter had a mad tea party <end>\",\n",
    "    \"the queen of hearts shouted <end>\",\n",
    "    \"alice was very curious <end>\",\n",
    "    \"the cat sat on a branch <end>\",\n",
    "    \"the hatter and the march hare <end>\",\n",
    "    \"who are you said the caterpillar <end>\",\n",
    "    \"i am alice said alice <end>\",\n",
    "    \"the queen played croquet <end>\"\n",
    "]\n",
    "\n",
    "# 2. Supervised Fine-Tuning (SFT) Dataset: Q&A pairs\n",
    "#    This teaches the model to be a helpful \"Alice\" expert.\n",
    "sft_dataset = {\n",
    "    # Prompt: \"Ideal Answer\"\n",
    "    \"who did alice follow\": \"alice followed the white rabbit <end>\",\n",
    "    \"where did alice go\": \"alice went down the rabbit hole <end>\",\n",
    "    \"tell me about the rabbit\": \"the white rabbit was late <end>\",\n",
    "    \"what did the cat do\": \"the cheshire cat smiled <end>\",\n",
    "    \"what about the hatter\": \"the hatter had a mad tea party <end>\",\n",
    "    \"what did the queen do\": \"the queen of hearts shouted <end>\"\n",
    "}\n",
    "\n",
    "# 3. RLHF Prompts: Prompts for the \"preference\" phase.\n",
    "rlhf_prompts = [\n",
    "    \"who did alice follow\",\n",
    "    \"tell me about the cat\",\n",
    "    \"what did the queen do\",\n",
    "    \"who was alice\",  # Note: This is a new question!\n",
    "    \"tell me about the hatter\"\n",
    "]\n",
    "\n",
    "# Create the \"full\" corpus for the tokenizer\n",
    "full_corpus = pretrain_corpus + list(sft_dataset.keys()) + list(sft_dataset.values())\n",
    "print(f\"Total pre-training sentences: {len(pretrain_corpus)}\")\n",
    "print(f\"Total SFT pairs: {len(sft_dataset)}\")\n",
    "print(f\"Total RLHF prompts: {len(rlhf_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c156253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing the Tokenizer ---\n",
      "Vocabulary Size: 50\n",
      "Vocabulary: {'<end>': 0, 'a': 1, 'about': 2, 'alice': 3, 'am': 4, 'and': 5, 'are': 6, 'branch': 7, 'cat': 8, 'caterpillar': 9, 'cheshire': 10, 'croquet': 11, 'curious': 12, 'did': 13, 'do': 14, 'down': 15, 'follow': 16, 'followed': 17, 'go': 18, 'had': 19, 'hare': 20, 'hatter': 21, 'hearts': 22, 'hole': 23, 'i': 24, 'late': 25, 'mad': 26, 'march': 27, 'me': 28, 'of': 29, 'on': 30, 'party': 31, 'played': 32, 'queen': 33, 'rabbit': 34, 'said': 35, 'sat': 36, 'shouted': 37, 'smiled': 38, 'tea': 39, 'tell': 40, 'the': 41, 'very': 42, 'was': 43, 'went': 44, 'what': 45, 'where': 46, 'white': 47, 'who': 48, 'you': 49}\n",
      "\n",
      "Original text: 'curious white cat and rabbit <end>'\n",
      "Encoded IDs: [12, 47, 8, 5, 34, 0]\n",
      "Decoded text: 'curious white cat and rabbit <end>'\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Concept 1: Tokenization (The Vocabulary) ---\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    A simple word-level tokenizer. It learns a vocabulary from a\n",
    "    corpus and can convert text to a sequence of integer IDs\n",
    "    (encode) and back (decode).\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus):\n",
    "        # 1. Find all unique words in the corpus\n",
    "        # We split every sentence, flatten the list, and use a 'set'\n",
    "        # to get only the unique words.\n",
    "        words = sorted(list(set(word for sentence in corpus for word in sentence.split())))\n",
    "        \n",
    "        # 2. Create the mapping dictionaries\n",
    "        # word_to_idx: Maps a word (e.g., \"sky\") to an integer (e.g., 5)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        # idx_to_word: Maps an integer (e.g., 5) back to a word (e.g., \"sky\")\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        \n",
    "        # 3. Store the vocabulary size\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Converts a string of text into a list of token IDs.\"\"\"\n",
    "        return [self.word_to_idx[word] for word in text.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Converts a list of token IDs back into a string of text.\"\"\"\n",
    "        return ' '.join([self.idx_to_word[token] for token in tokens])\n",
    "\n",
    "# --- Let's test our Tokenizer! ---\n",
    "print(\"--- Testing the Tokenizer ---\")\n",
    "\n",
    "# 1. Initialize the tokenizer on our full dataset\n",
    "tokenizer = SimpleTokenizer(full_corpus)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {tokenizer.word_to_idx}\")\n",
    "\n",
    "# 2. Test encoding\n",
    "text = \"curious white cat and rabbit <end>\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"\\nOriginal text: '{text}'\")\n",
    "print(f\"Encoded IDs: {encoded}\")\n",
    "\n",
    "# 3. Test decoding\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded text: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff167acf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4eae6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing the Embedding Layer ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sun'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Testing the Embedding Layer ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 1. Get the ID for the word \"sun\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m sun_id \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msun\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 2. Get the embedding vector for \"sun\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# We wrap the ID in a torch.tensor\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sun'"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Concept 2: Word Embeddings (The Meaning) ---\n",
    "\n",
    "# Define the \"dimensionality\" of our embeddings.\n",
    "# This is a hyperparameter: a choice we make.\n",
    "# A bigger number can capture more meaning, but is slower.\n",
    "# Real models use dimensions like 768 or 4096. We'll use 10.\n",
    "EMBED_DIM = 10\n",
    "\n",
    "# 1. Create the embedding layer\n",
    "# It's a simple lookup table: vocab_size (rows) x embed_dim (columns)\n",
    "# It's like a big spreadsheet where row 5 is the vector for word ID 5.\n",
    "embedding_layer = nn.Embedding(vocab_size, EMBED_DIM)\n",
    "\n",
    "# --- Let's test the Embedding Layer! ---\n",
    "print(\"--- Testing the Embedding Layer ---\")\n",
    "\n",
    "# 1. Get the ID for the word \"sun\"\n",
    "sun_id = tokenizer.word_to_idx['sun']\n",
    "print(f\"ID for 'sun': {sun_id}\")\n",
    "\n",
    "# 2. Get the embedding vector for \"sun\"\n",
    "# We wrap the ID in a torch.tensor\n",
    "sun_id_tensor = torch.tensor([sun_id])\n",
    "sun_vector = embedding_layer(sun_id_tensor)\n",
    "\n",
    "print(f\"\\nUntrained 'sun' vector (shape {sun_vector.shape}):\")\n",
    "print(sun_vector)\n",
    "\n",
    "print(\"\\nNote: These numbers are random. During training, the model will learn\")\n",
    "print(\"to make the vectors for 'sun' and 'hot' similar, and different\")\n",
    "print(\"from the vectors for 'sky' and 'blue'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6978b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyLLM class defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Concept 3: The Neural Network (The \"Brain\") ---\n",
    "\n",
    "# We define our model as a Python class that inherits from PyTorch's\n",
    "# nn.Module, which is the base class for all neural network modules.\n",
    "\n",
    "class ToyLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        This is the constructor. It sets up the layers our model\n",
    "        will use. It doesn't define how data flows, just the\n",
    "        building blocks.\n",
    "        \"\"\"\n",
    "        super().__init__() # Always call this first\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Layer 1: The Embedding Layer\n",
    "        # (From Cell 4) Converts token IDs -> meaning vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Layer 2: The \"Memory\" Layer (LSTM)\n",
    "        # This layer processes the sequence of embedding vectors.\n",
    "        # 'hidden_dim' is the size of its \"memory\" or \"thought\" vector.\n",
    "        # 'batch_first=True' just means our input data will have\n",
    "        # the batch size as the first dimension (e.g., [batch, seq_len, features])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Layer 3: The \"Decision\" Layer (Fully Connected)\n",
    "        # This layer takes the final \"thought\" from the LSTM and maps\n",
    "        # it to a score for every single word in our vocabulary.\n",
    "        # The word with the highest score is the model's prediction.\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the 'forward pass'. It defines how data flows\n",
    "        through the layers we defined in __init__.\n",
    "        \n",
    "        'x' will be our input tensor of token IDs (e.g., [1, 5, 4])\n",
    "        \"\"\"\n",
    "        # 1. Get Embeddings\n",
    "        # x shape: [batch_size, sequence_length]\n",
    "        # embedded shape: [batch_size, sequence_length, embed_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # 2. Process with LSTM\n",
    "        # lstm_out shape: [batch_size, sequence_length, hidden_dim]\n",
    "        # 'hidden_state' (which we ignore with '_') would be the\n",
    "        # final memory state.\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # 3. Get the *last* token's output\n",
    "        # We only care about the LSTM's \"thought\" *after* it has read\n",
    "        # the entire input sequence. So, we select the output\n",
    "        # corresponding to the last token (index -1).\n",
    "        # last_token_out shape: [batch_size, hidden_dim]\n",
    "        last_token_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 4. Make a prediction (get \"logits\")\n",
    "        # 'output' will be a vector of scores, one for each word\n",
    "        # in the vocabulary.\n",
    "        # output shape: [batch_size, vocab_size]\n",
    "        output = self.fc(last_token_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"ToyLLM class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1212d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_text() helper function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Helper Function (Generating Text) ---\n",
    "\n",
    "def generate_text(model, tokenizer, seed_text, max_len=10):\n",
    "    \"\"\"\n",
    "    Generates text from a model, starting with a seed_text.\n",
    "    \"\"\"\n",
    "    # 1. Set the model to \"evaluation mode\"\n",
    "    # This turns off things like dropout, which are only used\n",
    "    # during training.\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Tokenize the starting text\n",
    "    tokens = tokenizer.encode(seed_text)\n",
    "    \n",
    "    # 3. Generation loop\n",
    "    for _ in range(max_len):\n",
    "        # 4. Convert tokens to a tensor\n",
    "        # The model expects a \"batch\", so we add an\n",
    "        # extra dimension with []\n",
    "        input_tensor = torch.tensor([tokens])\n",
    "        \n",
    "        # 5. Run the model\n",
    "        # torch.no_grad() tells PyTorch we don't need to\n",
    "        # calculate gradients, which saves memory and time.\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        # 6. Get the prediction\n",
    "        # 'output' is a tensor of scores (logits).\n",
    "        # output.argmax(1) finds the index (the token ID)\n",
    "        # with the highest score.\n",
    "        # .item() converts the tensor to a plain Python number.\n",
    "        next_token = output.argmax(1).item()\n",
    "        \n",
    "        # 7. Add the new token to our list\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        # 8. Stop if we predict the <end> token\n",
    "        if tokenizer.decode([next_token]) == '<end>':\n",
    "            break\n",
    "            \n",
    "    # 9. Decode the full list of tokens back into a string\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "print(\"generate_text() helper function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31acad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 50 vocab, 10 embed dim, 20 hidden dim.\n",
      "\n",
      "--- Testing Untrained Model ---\n",
      "Prompt: 'the rabbit is' -> Response: 'the rabbit where where hearts where hearts where hearts where hearts where'\n",
      "\n",
      "(Note: The output is random garbage, as expected!)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Initialization & First Test ---\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# We use the constants we defined earlier\n",
    "# EMBED_DIM = 10\n",
    "HIDDEN_DIM = 20 # Size of the LSTM's \"memory\"\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# --- Initialize the Model ---\n",
    "toy_model = ToyLLM(vocab_size=vocab_size, \n",
    "                   embed_dim=EMBED_DIM, \n",
    "                   hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "# --- Initialize the \"Optimizer\" and \"Loss Function\" ---\n",
    "# The Optimizer (e.g., Adam) is what updates the model's\n",
    "# weights. It needs to know the learning rate.\n",
    "optimizer = optim.Adam(toy_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# The Loss Function (CrossEntropyLoss) measures how \"wrong\" the\n",
    "# model's prediction is compared to the correct answer. It's\n",
    "# perfect for classification, and \"next-word prediction\" is just\n",
    "# a classification problem with 'vocab_size' classes.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model initialized with {vocab_size} vocab, {EMBED_DIM} embed dim, {HIDDEN_DIM} hidden dim.\")\n",
    "\n",
    "# --- Test the UNTRAINED model ---\n",
    "print(\"\\n--- Testing Untrained Model ---\")\n",
    "# The seed_text must only contain words from our vocabulary\n",
    "seed_text = \"the rabbit\" \n",
    "generated = generate_text(toy_model, tokenizer, seed_text)\n",
    "print(f\"Prompt: '{seed_text} is' -> Response: '{generated}'\")\n",
    "print(\"\\n(Note: The output is random garbage, as expected!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd605e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Phase 1: Foundational Pre-training ---\n",
      "Goal: Teach the model basic language structure by predicting the next word.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pre-training Epoch 10/80, Loss: 0.3238\n",
      "  Pre-training Epoch 20/80, Loss: 0.3201\n",
      "  Pre-training Epoch 30/80, Loss: 0.3189\n",
      "  Pre-training Epoch 40/80, Loss: 0.3159\n",
      "  Pre-training Epoch 50/80, Loss: 0.3132\n",
      "  Pre-training Epoch 60/80, Loss: 0.3112\n",
      "  Pre-training Epoch 70/80, Loss: 0.3110\n",
      "  Pre-training Epoch 80/80, Loss: 0.3099\n",
      "--- Pre-training Complete ---\n",
      "\n",
      "--- Testing Model After Pre-training ---\n",
      "Prompt: 'alice followed' -> Response: 'alice followed the white rabbit <end>'\n",
      "Prompt: 'the queen of' -> Response: 'the queen of hearts shouted <end>'\n",
      "Prompt: 'who did alice follow' -> Response: 'who did alice follow the white rabbit <end>'\n",
      "\n",
      "(Note: It's learned 'Alice' facts, but still just completes. It doesn't *answer*.)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Phase 1: Foundational Pre-training ---\n",
    "\n",
    "def simulate_pretraining(model, tokenizer, corpus, epochs=50):\n",
    "    print(\"\\n--- Starting Phase 1: Foundational Pre-training ---\")\n",
    "    print(\"Goal: Teach the model basic language structure by predicting the next word.\")\n",
    "    \n",
    "    # 1. Create the training data\n",
    "    # We turn each sentence into multiple (input, target) pairs.\n",
    "    # e.g., \"the sun is <end>\" becomes:\n",
    "    # ([\"the\"], \"sun\")\n",
    "    # ([\"the\", \"sun\"], \"is\")\n",
    "    # ([\"the\", \"sun\", \"is\"], \"<end>\")\n",
    "    inputs, targets = [], []\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer.encode(sentence)\n",
    "        for i in range(1, len(tokens)):\n",
    "            inputs.append(tokens[:i])   # The sequence so far\n",
    "            targets.append(tokens[i])  # The *next* word\n",
    "            \n",
    "    # --- The Training Loop ---\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # We'll just iterate through our simple dataset\n",
    "        for i in range(len(inputs)):\n",
    "            # Get the current (input, target) pair\n",
    "            input_seq = torch.tensor([inputs[i]])\n",
    "            target_val = torch.tensor([targets[i]])\n",
    "            \n",
    "            # --- Standard PyTorch Training Steps ---\n",
    "            \n",
    "            # 1. Reset gradients\n",
    "            # We must do this every time, or they will accumulate\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. Forward pass: Get model's prediction\n",
    "            output = model(input_seq)\n",
    "            \n",
    "            # 3. Calculate loss: How \"wrong\" was the prediction?\n",
    "            # 'output' shape: [1, vocab_size]\n",
    "            # 'target_val' shape: [1]\n",
    "            loss = criterion(output, target_val)\n",
    "            \n",
    "            # 4. Backward pass: Calculate gradients\n",
    "            # This is where PyTorch figures out how much each\n",
    "            # model parameter contributed to the error.\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. Optimizer step: Update model parameters\n",
    "            # The optimizer \"nudges\" the weights in the correct\n",
    "            # direction to reduce the loss.\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Pre-training Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(inputs):.4f}\")\n",
    "\n",
    "    print(\"--- Pre-training Complete ---\")\n",
    "    return model\n",
    "# --- [Find this line at the bottom of Cell 8] ---\n",
    "# ... (rest of the cell is the same) ...\n",
    "\n",
    "# --- Run Phase 1 ---\n",
    "# We pass in the 'toy_model' we already created\n",
    "# INCREASED EPOCHS for the larger corpus\n",
    "base_model = simulate_pretraining(toy_model, tokenizer, pretrain_corpus, epochs=80) \n",
    "\n",
    "# --- Test the Pre-trained model ---\n",
    "print(\"\\n--- Testing Model After Pre-training ---\")\n",
    "seed_text_1 = \"alice followed\"\n",
    "print(f\"Prompt: '{seed_text_1}' -> Response: '{generate_text(base_model, tokenizer, seed_text_1)}'\")\n",
    "\n",
    "seed_text_2 = \"the queen of\"\n",
    "print(f\"Prompt: '{seed_text_2}' -> Response: '{generate_text(base_model, tokenizer, seed_text_2)}'\")\n",
    "\n",
    "seed_text_3 = \"who did alice follow\" # Note: This is a QUESTION\n",
    "print(f\"Prompt: '{seed_text_3}' -> Response: '{generate_text(base_model, tokenizer, seed_text_3)}'\")\n",
    "print(\"\\n(Note: It's learned 'Alice' facts, but still just completes. It doesn't *answer*.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2af953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Phase 2: Supervised Fine-Tuning (SFT) ---\n",
      "Goal: Teach the model to follow instructions in a question-answer format.\n",
      "  SFT Epoch 10/50, Loss: 0.5940\n",
      "  SFT Epoch 20/50, Loss: 0.2010\n",
      "  SFT Epoch 30/50, Loss: 0.0968\n",
      "  SFT Epoch 40/50, Loss: 0.0572\n",
      "  SFT Epoch 50/50, Loss: 0.0374\n",
      "--- SFT Complete ---\n",
      "\n",
      "--- Testing Model After SFT ---\n",
      "Prompt: 'where did alice go' -> Response: 'where did alice go alice went down the rabbit hole <end>'\n",
      "Prompt: 'what did the cat do' -> Response: 'what did the cat do the cheshire cat smiled <end>'\n",
      "Prompt: 'who was alice' -> Response: 'who was alice followed the white rabbit was late <end>'\n",
      "\n",
      "(Note: It's great at SFT examples, but for new questions, it gives the pre-trained answer.)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 9: Phase 2: Supervised Fine-Tuning (SFT) ---\n",
    "\n",
    "def simulate_sft(model, tokenizer, sft_data, epochs=30):\n",
    "    print(\"\\n--- Starting Phase 2: Supervised Fine-Tuning (SFT) ---\")\n",
    "    print(\"Goal: Teach the model to follow instructions in a question-answer format.\")\n",
    "    \n",
    "    # We'll use a slightly smaller learning rate for fine-tuning\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over our Q&A pairs\n",
    "        for prompt, ideal_response in sft_data.items():\n",
    "            \n",
    "            input_tokens = tokenizer.encode(prompt)\n",
    "            target_tokens = tokenizer.encode(ideal_response)\n",
    "            \n",
    "            # \"Teacher Forcing\": We'll train the model to generate\n",
    "            # the response one word at a time, feeding it the\n",
    "            # correct sequence so far.\n",
    "            \n",
    "            # e.g., for \"can you help me\": \"of course i can help <end>\"\n",
    "            # 1. Input: [prompt] + []\n",
    "            #    Target: \"of\"\n",
    "            # 2. Input: [prompt] + [\"of\"]\n",
    "            #    Target: \"course\"\n",
    "            # 3. ...and so on\n",
    "            \n",
    "            for i in range(len(target_tokens)):\n",
    "                # The input is the prompt + the ideal response *so far*\n",
    "                current_input = torch.tensor([input_tokens + target_tokens[:i]])\n",
    "                # The target is the *next* word in the ideal response\n",
    "                current_target = torch.tensor([target_tokens[i]])\n",
    "                \n",
    "                # --- Standard Training Steps ---\n",
    "                optimizer.zero_grad()\n",
    "                output = model(current_input)\n",
    "                loss = criterion(output, current_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  SFT Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(sft_data):.4f}\")\n",
    "            \n",
    "    print(\"--- SFT Complete ---\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Run Phase 2 ---\n",
    "# We pass in the 'base_model' from Phase 1\n",
    "# INCREASED EPOCHS for the new SFT data\n",
    "sft_model = simulate_sft(base_model, tokenizer, sft_dataset, epochs=50)\n",
    "\n",
    "# --- Test the SFT model ---\n",
    "print(\"\\n--- Testing Model After SFT ---\")\n",
    "seed_text_1 = \"where did alice go\" # Was in SFT data\n",
    "print(f\"Prompt: '{seed_text_1}' -> Response: '{generate_text(sft_model, tokenizer, seed_text_1)}'\")\n",
    "\n",
    "seed_text_2 = \"what did the cat do\" # Was in SFT data\n",
    "print(f\"Prompt: '{seed_text_2}' -> Response: '{generate_text(sft_model, tokenizer, seed_text_2)}'\")\n",
    "\n",
    "seed_text_3 = \"who was alice\" # Was NOT in SFT data (pre-trained \"alice was very curious\")\n",
    "print(f\"Prompt: '{seed_text_3}' -> Response: '{generate_text(sft_model, tokenizer, seed_text_3)}'\")\n",
    "print(\"\\n(Note: It's great at SFT examples, but for new questions, it gives the pre-trained answer.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c2fa31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF functions defined with NEW 'Alice' reward model.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 10: Phase 3: Reinforcement Learning (RLHF) ---\n",
    "\n",
    "# --- Step 3a: Simulate a NEW Reward Model (The \"Alice\" Judge) ---\n",
    "def get_reward_score(response):\n",
    "    \"\"\"\n",
    "    A new, rule-based reward model that understands \"Alice\" facts.\n",
    "    \n",
    "    This \"judge\" prefers helpful and factually-correct answers\n",
    "    from the story.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # --- Positive Rewards (Factual & Helpful) ---\n",
    "    # Prefers the *SFT answers*\n",
    "    if \"alice followed the white rabbit\" in response: score += 1.5\n",
    "    if \"the cheshire cat smiled\" in response: score += 1.5\n",
    "    if \"the hatter had a mad tea party\" in response: score += 1.5\n",
    "    if \"the queen of hearts shouted\" in response: score += 1.0\n",
    "    \n",
    "    # Also rewards good *pre-training* facts\n",
    "    if \"alice was very curious\" in response: score += 1.0\n",
    "    \n",
    "    # --- Negative Penalties (Unhelpful or Wrong) ---\n",
    "    if \"i am a bot\" in response: score -= 2.0     # Penalizes robotic answers\n",
    "    if \"i do not know\" in response: score -= 1.0  # Penalizes unhelpful\n",
    "    \n",
    "    # Penalize \"factual errors\" (e.g., mixing up characters)\n",
    "    if \"alice smiled\" in response: score -= 1.5\n",
    "    if \"the cat followed the rabbit\" in response: score -= 2.0\n",
    "        \n",
    "    if len(response.split()) < 4: score -= 1.0    # Penalizes short answers\n",
    "    return score\n",
    "\n",
    "# --- Step 3b: Fine-tune with Reinforcement Learning ---\n",
    "# (The simulate_rlhf function code is IDENTICAL to before, no change needed)\n",
    "def simulate_rlhf(model, tokenizer, prompts, iterations=50):\n",
    "    print(\"\\n--- Starting Phase 3: Reinforcement Learning with Human Feedback (RLHF) ---\")\n",
    "    print(\"Goal: Refine the model based on preferences (what makes a 'good' answer).\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        prompt = random.choice(prompts)\n",
    "        response_str = generate_text(model, tokenizer, prompt, max_len=15)\n",
    "        reward = get_reward_score(response_str)\n",
    "        \n",
    "        log_probs = []\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "        response_tokens = tokenizer.encode(response_str.replace(prompt, '').strip())\n",
    "        \n",
    "        if not response_tokens:\n",
    "            continue\n",
    "            \n",
    "        for token in response_tokens:\n",
    "            input_tensor = torch.tensor([input_tokens])\n",
    "            output = model(input_tensor)\n",
    "            log_prob_dist = torch.log_softmax(output, dim=1)\n",
    "            log_prob = log_prob_dist[0, token]\n",
    "            log_probs.append(log_prob)\n",
    "            input_tokens.append(token)\n",
    "            \n",
    "        if log_probs:\n",
    "            policy_loss = -torch.stack(log_probs).mean() * reward\n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (i + 1) % (iterations // 5) == 0: # Print 5 times\n",
    "            print(f\"  RLHF Iteration {i+1}/{iterations}, Prompt: '{prompt}', Reward: {reward:.2f}\")\n",
    "\n",
    "    print(\"--- RLHF Complete ---\")\n",
    "    return model\n",
    "\n",
    "print(\"RLHF functions defined with NEW 'Alice' reward model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2466b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Phase 3: Reinforcement Learning with Human Feedback (RLHF) ---\n",
      "Goal: Refine the model based on preferences (what makes a 'good' answer).\n",
      "  RLHF Iteration 20/100, Prompt: 'tell me about the cat', Reward: 0.00\n",
      "  RLHF Iteration 40/100, Prompt: 'tell me about the hatter', Reward: 1.50\n",
      "  RLHF Iteration 60/100, Prompt: 'tell me about the cat', Reward: 0.00\n",
      "  RLHF Iteration 80/100, Prompt: 'who did alice follow', Reward: 1.50\n",
      "  RLHF Iteration 100/100, Prompt: 'tell me about the hatter', Reward: 1.50\n",
      "--- RLHF Complete ---\n",
      "\n",
      "--- Final Model after RLHF ---\n",
      "Note how the model's behavior has been 'steered' by the 'Alice' reward model.\n",
      "Prompt: 'what did the cat do' -> Response: 'what did the cat do the cheshire cat smiled <end>'\n",
      "Prompt: 'what about the hatter' -> Response: 'what about the hatter the hatter had a mad tea party <end>'\n",
      "Prompt: 'who was alice' -> Response: 'who was alice followed the white rabbit was late <end>'\n",
      "\n",
      "--- Talk Summary ---\n",
      "1. UNTRAINED: Random garbage.\n",
      "2. PRE-TRAINED: Knew 'Alice' facts, but just completed sentences.\n",
      "3. SFT: Knew how to *answer specific questions* (e.g., 'the cheshire cat smiled').\n",
      "4. RLHF: Learned *general preferences*. It learned that 'alice was very curious' is a\n",
      "   'good' answer to 'who was alice', even without SFT. It was steered by the reward!\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 11: Final Run & The \"Aligned\" Model ---\n",
    "\n",
    "# --- Run Phase 3 ---\n",
    "# We pass in the 'sft_model' from Phase 2\n",
    "# INCREASED ITERATIONS for more preference learning\n",
    "rlhf_model = simulate_rlhf(sft_model, tokenizer, rlhf_prompts, iterations=100) \n",
    "\n",
    "# --- Test the FINAL model ---\n",
    "print(\"\\n--- Final Model after RLHF ---\")\n",
    "print(\"Note how the model's behavior has been 'steered' by the 'Alice' reward model.\")\n",
    "\n",
    "seed_text_1 = \"what did the cat do\" # (SFT, high-reward)\n",
    "print(f\"Prompt: '{seed_text_1}' -> Response: '{generate_text(rlhf_model, tokenizer, seed_text_1)}'\")\n",
    "\n",
    "seed_text_2 = \"what about the hatter\" # (SFT, high-reward)\n",
    "print(f\"Prompt: '{seed_text_2}' -> Response: '{generate_text(rlhf_model, tokenizer, seed_text_2)}'\")\n",
    "\n",
    "# --- The KEY Test ---\n",
    "seed_text_3 = \"who was alice\" # (Not in SFT, but \"alice was very curious\" is high-reward)\n",
    "print(f\"Prompt: '{seed_text_3}' -> Response: '{generate_text(rlhf_model, tokenizer, seed_text_3)}'\")\n",
    "\n",
    "print(\"\\n--- Talk Summary ---\")\n",
    "print(\"1. UNTRAINED: Random garbage.\")\n",
    "print(\"2. PRE-TRAINED: Knew 'Alice' facts, but just completed sentences.\")\n",
    "print(\"3. SFT: Knew how to *answer specific questions* (e.g., 'the cheshire cat smiled').\")\n",
    "print(\"4. RLHF: Learned *general preferences*. It learned that 'alice was very curious' is a\")\n",
    "print(\"   'good' answer to 'who was alice', even without SFT. It was steered by the reward!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7f7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 44\n",
      "Sequence Length (context window): 4\n",
      "Total training examples: 53\n",
      "Starting pre-training with 3000 epochs...\n",
      "Epoch [500/3000], Loss: 0.1051\n",
      "Epoch [1000/3000], Loss: 0.0786\n",
      "Epoch [1500/3000], Loss: 0.0785\n",
      "Epoch [2000/3000], Loss: 0.0785\n",
      "Epoch [2500/3000], Loss: 0.0785\n",
      "Epoch [3000/3000], Loss: 0.0785\n",
      "Pre-training complete.\n",
      "\n",
      "--- Generating Text (Max 20 tokens) ---\n",
      "Start: 'alice was beginning to get'\n",
      "\n",
      "Resulting Text:\n",
      "alice was beginning to get very tired of sitting nothing to do: once or twice she had peeped into the book her sister on\n",
      "\n",
      "--- ATTENTION MECHANISM SUMMARY ---\n",
      "In the SelfAttention class, the key operation is:\n",
      "1. Q, K, V Projections: Maps input vector (x) into three roles.\n",
      "2. Scoring: torch.matmul(Q, K.transpose) calculates pairwise similarity.\n",
      "3. Weighting: Softmax turns these scores into probabilistic attention weights.\n",
      "4. Context: torch.matmul(Weights, V) sums the V vectors based on the computed weights.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# --- Data Preparation ---\n",
    "\n",
    "# A simple, small corpus from Alice in Wonderland\n",
    "corpus = \"\"\"\n",
    "alice was beginning to get very tired of sitting by her sister on the bank,\n",
    "and of having nothing to do: once or twice she had peeped into the book her\n",
    "sister was reading, but it had no pictures or conversations in it, and what\n",
    "is the use of a book, thought alice without pictures or conversations?\n",
    "\"\"\"\n",
    "\n",
    "# Tokenization and Vocabulary\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, text):\n",
    "        # Convert all text to lower case and split by whitespace\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        # Create a set of unique words\n",
    "        vocab = sorted(list(set(tokens)))\n",
    "        \n",
    "        # Map words to unique integers (token IDs)\n",
    "        self.word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab) + 1 # +1 for padding/unknown token (index 0)\n",
    "\n",
    "    def encode(self, text, max_len=None):\n",
    "        tokens = text.lower().split()\n",
    "        encoded = [self.word_to_idx.get(word, 0) for word in tokens]\n",
    "        \n",
    "        if max_len:\n",
    "            # Pad or truncate the sequence\n",
    "            if len(encoded) < max_len:\n",
    "                encoded += [0] * (max_len - len(encoded))\n",
    "            else:\n",
    "                encoded = encoded[:max_len]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        # Decode only if the index is not 0 (padding/unknown)\n",
    "        return ' '.join([self.idx_to_word[idx] for idx in encoded if idx != 0 and idx in self.idx_to_word])\n",
    "\n",
    "# Create sequences for next-word prediction\n",
    "def create_sequences(tokenizer, corpus, seq_len=4):\n",
    "    tokens = corpus.lower().split()\n",
    "    encoded_tokens = tokenizer.encode(corpus)\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(encoded_tokens) - seq_len):\n",
    "        # Input: sequence of length seq_len\n",
    "        input_seq = encoded_tokens[i:i + seq_len]\n",
    "        # Target: the token immediately following the input sequence\n",
    "        target_token = encoded_tokens[i + seq_len]\n",
    "        \n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_token)\n",
    "        \n",
    "    return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "# --- Transformer Core Component: Scaled Dot-Product Self-Attention ---\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified single-head Self-Attention mechanism, the heart of the Transformer.\n",
    "    This replaces the sequential processing of an RNN/LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear layers to project the input embedding into Query, Key, and Value vectors\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # 1. Project to Q, K, V\n",
    "        Q = self.W_q(x) # Query: What am I looking for?\n",
    "        K = self.W_k(x) # Key: What do I have?\n",
    "        V = self.W_v(x) # Value: What context should I pass?\n",
    "        \n",
    "        # 2. Compute Attention Scores (Scaled Dot-Product)\n",
    "        # Q @ K.transpose(-2, -1) calculates the similarity between all pairs of words.\n",
    "        # Scaling by sqrt(d_k) stabilizes the gradient.\n",
    "        d_k = Q.size(-1)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # 3. Normalize Scores\n",
    "        # Softmax turns scores into probability weights (0 to 1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 4. Apply Weights to Values\n",
    "        # The output is a weighted sum of the Value vectors (Context Vector)\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return context_vector # shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "\n",
    "# --- The Toy LLM using Attention ---\n",
    "\n",
    "class ToyAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal LLM architecture using an embedding layer followed by Self-Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 1. Embedding Layer: Converts token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 2. Positional Encoding (Crucial for Transformers)\n",
    "        # Since attention processes all words at once, we must inject their order.\n",
    "        self.positional_encoding = nn.Embedding(seq_len, embed_dim)\n",
    "\n",
    "        # 3. Self-Attention Block (The new core)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "\n",
    "        # 4. Final Linear Layer: Maps the context vector back to the vocab size\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # 1. Look up word embeddings\n",
    "        word_embeddings = self.embedding(x) # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # 2. Add Positional Encoding\n",
    "        # Generate position indices (0, 1, 2, 3...)\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embeddings = self.positional_encoding(positions)\n",
    "        \n",
    "        # Transformer Input = Word Embedding + Positional Encoding\n",
    "        x = word_embeddings + pos_embeddings\n",
    "        \n",
    "        # 3. Pass through Self-Attention\n",
    "        context_vector = self.attention(x) # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # For next-word prediction, we only care about the last word's context\n",
    "        # to predict the next word in the sequence.\n",
    "        last_context = context_vector[:, -1, :] # (batch_size, embed_dim)\n",
    "\n",
    "        # 4. Output: Predict the next token\n",
    "        output = self.fc(last_context) # (batch_size, vocab_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# --- Training and Inference Functions ---\n",
    "\n",
    "def simulate_pretraining(model, inputs, targets, epochs=3000): # Increased epochs\n",
    "    \"\"\"\n",
    "    Simulates the foundational pre-training phase (Next-Word Prediction).\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    print(f\"Starting pre-training with {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        # 1. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2. Calculate Loss (Error)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 3. Backpropagation (Find Blame)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Optimization (Adjust Weights)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    print(\"Pre-training complete.\")\n",
    "    return model\n",
    "\n",
    "def generate_text(model, tokenizer, start_phrase, seq_len=4, max_tokens=10, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generates text using the trained model, using temperature sampling\n",
    "    to prevent getting stuck in repetitive loops.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    generated_tokens = tokenizer.encode(start_phrase, max_len=seq_len)\n",
    "    \n",
    "    if len(generated_tokens) < seq_len:\n",
    "        print(\"Error: Start phrase is too short for the required sequence length.\")\n",
    "        return start_phrase\n",
    "\n",
    "    print(f\"\\n--- Generating Text (Max {max_tokens} tokens) ---\")\n",
    "    print(f\"Start: '{start_phrase}'\")\n",
    "    \n",
    "    output_tokens = generated_tokens\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculations during inference\n",
    "        for _ in range(max_tokens):\n",
    "            # 1. Prepare the current sequence input\n",
    "            current_sequence = torch.tensor([output_tokens[-seq_len:]])\n",
    "            \n",
    "            # 2. Get prediction from the Attention Model\n",
    "            output = model(current_sequence) # (1, vocab_size)\n",
    "            \n",
    "            # 3. Apply Temperature (dividing logits by T)\n",
    "            output = output / temperature\n",
    "            \n",
    "            # 4. Convert to probabilities and sample (instead of argmax)\n",
    "            probabilities = F.softmax(output, dim=-1)\n",
    "            predicted_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            \n",
    "            if predicted_token_id == 0: # Stop on padding/unknown\n",
    "                break\n",
    "            \n",
    "            # 5. Add the new token to the sequence for the next step\n",
    "            output_tokens.append(predicted_token_id)\n",
    "            \n",
    "            # Stop if the model starts repeating itself or generating gibberish\n",
    "            if len(output_tokens) > 2 * seq_len and len(set(output_tokens[-seq_len:])) < 2:\n",
    "                 break\n",
    "\n",
    "    # Decode and return the result\n",
    "    return tokenizer.decode(output_tokens)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define hyper-parameters\n",
    "    EMBED_DIM = 32    # Size of the word vector (Increased from 16)\n",
    "    SEQ_LEN = 4       # Number of words the model looks at to predict the next\n",
    "    EPOCHS = 3000     # Number of training iterations (Increased from 1500)\n",
    "\n",
    "    # 1. Data Setup\n",
    "    tokenizer = SimpleTokenizer(corpus)\n",
    "    VOCAB_SIZE = tokenizer.vocab_size\n",
    "    \n",
    "    inputs, targets = create_sequences(tokenizer, corpus, seq_len=SEQ_LEN)\n",
    "    \n",
    "    print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
    "    print(f\"Sequence Length (context window): {SEQ_LEN}\")\n",
    "    print(f\"Total training examples: {len(inputs)}\")\n",
    "\n",
    "    # 2. Model Initialization (Using the new Attention Model)\n",
    "    model = ToyAttentionModel(VOCAB_SIZE, EMBED_DIM, SEQ_LEN)\n",
    "    \n",
    "    # 3. Training\n",
    "    model = simulate_pretraining(model, inputs, targets, epochs=EPOCHS)\n",
    "    \n",
    "    # 4. Inference (Generate text)\n",
    "    start_phrase = \"alice was beginning to get\"\n",
    "    generated_text = generate_text(model, tokenizer, start_phrase, seq_len=SEQ_LEN, max_tokens=20)\n",
    "    \n",
    "    print(f\"\\nResulting Text:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Simple summary of the attention core\n",
    "    print(\"\\n--- ATTENTION MECHANISM SUMMARY ---\")\n",
    "    print(\"In the SelfAttention class, the key operation is:\")\n",
    "    print(\"1. Q, K, V Projections: Maps input vector (x) into three roles.\")\n",
    "    print(\"2. Scoring: torch.matmul(Q, K.transpose) calculates pairwise similarity.\")\n",
    "    print(\"3. Weighting: Softmax turns these scores into probabilistic attention weights.\")\n",
    "    print(\"4. Context: torch.matmul(Weights, V) sums the V vectors based on the computed weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8740c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-pytorch-env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
