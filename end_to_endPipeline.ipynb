{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff292bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# --- 0. Setup: A Toy Model and Helper Functions ---\n",
    "\n",
    "# A very simple language model for demonstration purposes\n",
    "class ToyLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # We only care about the output of the last token for next-word prediction\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Helper function to generate text from the model\n",
    "def generate_text(model, tokenizer, seed_text, max_len=10):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(seed_text)\n",
    "    input_tensor = torch.tensor([tokens]).to(next(model.parameters()).device)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            # Get the predicted next token (the one with the highest probability)\n",
    "            next_token = output.argmax(1).item()\n",
    "            tokens.append(next_token)\n",
    "            input_tensor = torch.tensor([tokens]).to(next(model.parameters()).device)\n",
    "            \n",
    "            if tokenizer.decode([next_token]) == '<end>':\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Simple tokenizer for our toy vocabulary\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        # Flatten the corpus and find unique words\n",
    "        words = sorted(list(set(word for sentence in corpus for word in sentence.split())))\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return [self.word_to_idx[word] for word in text.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.idx_to_word[token] for token in tokens])\n",
    "\n",
    "# --- 1. Phase 1: Foundational Pre-training ---\n",
    "def simulate_pretraining(model, tokenizer, corpus, epochs=50):\n",
    "    print(\"\\n--- Starting Phase 1: Foundational Pre-training ---\")\n",
    "    print(\"Goal: Teach the model basic language structure by predicting the next word.\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create input/output pairs for next-word prediction\n",
    "    inputs, targets = [], []\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer.encode(sentence)\n",
    "        for i in range(1, len(tokens)):\n",
    "            inputs.append(tokens[:i])\n",
    "            targets.append(tokens[i])\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_seq = torch.tensor([inputs[i]])\n",
    "            target_val = torch.tensor([targets[i]])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target_val)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Pre-training Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(inputs):.4f}\")\n",
    "\n",
    "    print(\"--- Pre-training Complete ---\")\n",
    "    return model\n",
    "\n",
    "# --- 2. Phase 2: Supervised Fine-Tuning (SFT) ---\n",
    "def simulate_sft(model, tokenizer, sft_data, epochs=30):\n",
    "    print(\"\\n--- Starting Phase 2: Supervised Fine-Tuning (SFT) ---\")\n",
    "    print(\"Goal: Teach the model to follow instructions in a question-answer format.\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for prompt, ideal_response in sft_data.items():\n",
    "            # The input is the prompt, and the target is the ideal response\n",
    "            input_tokens = tokenizer.encode(prompt)\n",
    "            target_tokens = tokenizer.encode(ideal_response)\n",
    "            \n",
    "            # We'll train the model to generate the response one word at a time\n",
    "            for i in range(len(target_tokens)):\n",
    "                current_input = torch.tensor([input_tokens + target_tokens[:i]])\n",
    "                current_target = torch.tensor([target_tokens[i]])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(current_input)\n",
    "                loss = criterion(output, current_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  SFT Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(sft_data):.4f}\")\n",
    "            \n",
    "    print(\"--- SFT Complete ---\")\n",
    "    return model\n",
    "\n",
    "# --- 3. Phase 3: Reinforcement Learning with Human Feedback (RLHF) ---\n",
    "\n",
    "# Step 3a: Simulate a Reward Model\n",
    "def get_reward_score(response):\n",
    "    \"\"\"\n",
    "    A simple, rule-based reward model. In reality, this would be a separate,\n",
    "    trained neural network.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    if \"i can help\" in response: score += 1.5 # Prefers helpfulness\n",
    "    if \"of course\" in response: score += 1.0\n",
    "    if \"sun is hot\" in response: score += 1.0 # Prefers factual correctness\n",
    "    if \"sky is blue\" in response: score += 1.0\n",
    "    if \"i am a bot\" in response: score -= 2.0 # Penalizes unhelpful or robotic answers\n",
    "    if len(response.split()) < 5: score -= 1.0 # Penalizes short, uninformative answers\n",
    "    return score\n",
    "\n",
    "# Step 3b: Fine-tune with Reinforcement Learning\n",
    "def simulate_rlhf(model, tokenizer, prompts, iterations=50):\n",
    "    print(\"\\n--- Starting Phase 3: Reinforcement Learning with Human Feedback (RLHF) ---\")\n",
    "    print(\"Goal: Refine the model based on preferences (what makes a 'good' answer).\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Pick a random prompt and generate a response\n",
    "        prompt = random.choice(prompts)\n",
    "        response_str = generate_text(model, tokenizer, prompt, max_len=15)\n",
    "        \n",
    "        # Get a score from our simulated reward model\n",
    "        reward = get_reward_score(response_str)\n",
    "        \n",
    "        # This is a simplified version of a policy gradient update (like PPO).\n",
    "        # We calculate a \"loss\" that is inversely proportional to the reward.\n",
    "        # A high reward means a low loss, and a low reward means a high loss.\n",
    "        # This encourages the model to generate responses that get high rewards.\n",
    "        loss = -reward \n",
    "        \n",
    "        # We need to backpropagate this \"loss\" through the generation process.\n",
    "        # This is complex, so we'll simulate it by treating the reward as a loss\n",
    "        # on the log probabilities of the generated tokens.\n",
    "        \n",
    "        # A simplified pseudo-loss calculation\n",
    "        log_probs = []\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "        response_tokens = tokenizer.encode(response_str.replace(prompt, '').strip())\n",
    "        \n",
    "        for token in response_tokens:\n",
    "            input_tensor = torch.tensor([input_tokens])\n",
    "            output = model(input_tensor)\n",
    "            log_prob = torch.log_softmax(output, dim=1)[0, token]\n",
    "            log_probs.append(log_prob)\n",
    "            input_tokens.append(token)\n",
    "            \n",
    "        if log_probs:\n",
    "            policy_loss = -torch.stack(log_probs).mean() * reward\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  RLHF Iteration {i+1}/{iterations}, Prompt: '{prompt}', Reward: {reward:.2f}\")\n",
    "\n",
    "    print(\"--- RLHF Complete ---\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Data and Vocabulary Setup ---\n",
    "    pretrain_corpus = [\n",
    "        \"the sun is hot <end>\",\n",
    "        \"the sky is blue <end>\",\n",
    "        \"i like to code <end>\",\n",
    "        \"what is your name <end>\",\n",
    "        \"my name is bot <end>\",\n",
    "        \"how can i help you <end>\",\n",
    "    ]\n",
    "    \n",
    "    # Add a special <end> token to our vocabulary\n",
    "    full_corpus = pretrain_corpus + [\"<end>\", \"of course i can help\", \"i am a bot\"]\n",
    "    tokenizer = SimpleTokenizer(full_corpus)\n",
    "    vocab_size = len(tokenizer.word_to_idx)\n",
    "    \n",
    "    sft_dataset = {\n",
    "        \"what is the sun\": \"the sun is hot <end>\",\n",
    "        \"what color is the sky\": \"the sky is blue <end>\",\n",
    "        \"can you help me\": \"of course i can help <end>\"\n",
    "    }\n",
    "    \n",
    "    rlhf_prompts = [\"can you help me\", \"what is your name\", \"what is the sun\"]\n",
    "\n",
    "    # --- Initialize the Model ---\n",
    "    toy_model = ToyLLM(vocab_size=vocab_size, embed_dim=10, hidden_dim=20)\n",
    "\n",
    "    # --- Run the Pipeline ---\n",
    "    print(\"--- Initial Untrained Model ---\")\n",
    "    print(f\"Prompt: 'the sun is' -> Response: '{generate_text(toy_model, tokenizer, 'the sun is')}'\")\n",
    "    \n",
    "    # Phase 1\n",
    "    base_model = simulate_pretraining(toy_model, tokenizer, pretrain_corpus)\n",
    "    print(\"\\n--- Model after Pre-training ---\")\n",
    "    print(f\"Prompt: 'the sun is' -> Response: '{generate_text(base_model, tokenizer, 'the sun is')}'\")\n",
    "    print(f\"Prompt: 'can you help me' -> Response: '{generate_text(base_model, tokenizer, 'can you help me')}'\")\n",
    "\n",
    "    # Phase 2\n",
    "    sft_model = simulate_sft(base_model, tokenizer, sft_dataset)\n",
    "    print(\"\\n--- Model after SFT ---\")\n",
    "    print(f\"Prompt: 'what is the sun' -> Response: '{generate_text(sft_model, tokenizer, 'what is the sun')}'\")\n",
    "    print(f\"Prompt: 'can you help me' -> Response: '{generate_text(sft_model, tokenizer, 'can you help me')}'\")\n",
    "    print(f\"Prompt: 'what is your name' -> Response: '{generate_text(sft_model, tokenizer, 'what is your name')}'\") # May still give a poor answer\n",
    "\n",
    "    # Phase 3\n",
    "    rlhf_model = simulate_rlhf(sft_model, tokenizer, rlhf_prompts)\n",
    "    print(\"\\n--- Final Model after RLHF ---\")\n",
    "    print(\"Note how the model now prefers the more 'helpful' sounding response.\")\n",
    "    print(f\"Prompt: 'what is the sun' -> Response: '{generate_text(rlhf_model, tokenizer, 'what is the sun')}'\")\n",
    "    print(f\"Prompt: 'can you help me' -> Response: '{generate_text(rlhf_model, tokenizer, 'can you help me')}'\")\n",
    "    print(f\"Prompt: 'what is your name' -> Response: '{generate_text(rlhf_model, tokenizer, 'what is your name')}'\") #"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
